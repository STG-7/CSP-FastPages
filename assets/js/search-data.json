{
  
    
        "post0": {
            "title": "Unit 2.2 Data Compression, Images",
            "content": "Enumerate &quot;Data&quot; Big Idea from College Board . Some of the big ideas and vocab that you observe, talk about it with a partner ... . &quot;Data compression is the reduction of the number of bits needed to represent data&quot; | &quot;Data compression is used to save transmission time and storage space.&quot; | &quot;lossy data can reduce data but the original data is not recovered&quot; | &quot;lossless data lets you restore and recover&quot; | . The Image Lab Project contains a plethora of College Board Unit 2 data concepts. Working with Images provides many opportunities for compression and analyzing size. . Image Files and Size . Here are some Images Files. Download these files, load them into images directory under _notebooks in your Blog. - Clouds Impression . Lassen Volcano | Green Square | . Describe some of the meta data and considerations when managing Image files. Describe how these relate to Data Compression ... . File Type, PNG and JPG are two types used in this lab: Not every file type is compatible for the same type of resizing, compression, etc. For instance, a jpg and png file may not use the same type of compression method. | Size, height and width, number of pixels: When managing image files, the height, width, and size can be adjusted when compressing. Later on in my hacks, we can see how I reduced the height and width of an original jpg file by 4x. | Visual perception, lossy compression: Lossy compression is when the file size of an image is compressed, and the visual perception experience of the image deteriorates; for instance, an image&#39;s quality becomes more fuzzy and less clear. | . Python Libraries and Concepts used for Jupyter and Files/Directories . Introduction to displaying images in Jupyter notebook . IPython . Support visualization of data in Jupyter notebooks. Visualization is specific to View, for the web visualization needs to be converted to HTML. . pathlib . File paths are different on Windows versus Mac and Linux. This can cause problems in a project as you work and deploy on different Operating Systems (OS&#39;s), pathlib is a solution to this problem. . What are commands you use in terminal to access files?:cd to change directory, ls to list the files in a directory, and cat to read the insides of a file through terminal.- What are the command you use in Windows terminal to access files? cd is still used to change directory and dir is more often used to list the files in a directory. | What are some of the major differences? Some of the commands to view files may be different, and Linux uses forward slashes while Windows uses backwards slashes, making the absolute directory paths different between the two OSs. | . Provide what you observed, struggled with, or leaned while playing with this code. . Why is path a big deal when working with images? Path is a big deal, as all images are stored in folders, and to access those images, the path has to be correct. | How does the meta data source and label relate to Unit 5 topics? Unit 5 was about programs that can analyze data. The meta data source and label relates to these topics, since metadata contains all of the unique information for an image, and the data can be outputted onto the screen from a program. | Look up IPython, describe why this is interesting in Jupyter Notebooks for both Pandas and Images? IPython is a Python interactive shell which can be utilized to display images. Furthermore, Pandas can be used as an imported module to extract certain metadata from these images. | . from IPython.display import Image, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images def image_display(images): for image in images: display(Image(filename=image[&#39;filename&#39;])) # Run this as standalone tester to see sample data printed in Jupyter terminal if __name__ == &quot;__main__&quot;: # print parameter supplied image green_square = image_data(images=[{&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}]) image_display(green_square) smiley = image_data(images=[{&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Smiley&quot;, &#39;file&#39;: &quot;smiley.png&quot;}]) image_display(smiley) # display default images from image_data() default_images = image_data() image_display(default_images) . Reading and Encoding Images (2 implementations follow) . PIL (Python Image Library) . Pillow or PIL provides the ability to work with images in Python. Geeks for Geeks shows some ideas on working with images. . base64 . Image formats (JPG, PNG) are often called *Binary File formats, it is difficult to pass these over HTTP. Thus, base64 converts binary encoded data (8-bit, ASCII/Unicode) into a text encoded scheme (24 bits, 6-bit Base64 digits). Thus base64 is used to transport and embed binary images into textual assets such as HTML and CSS.- How is Base64 similar or different to Binary and Hexadecimal?: Base64 is similar to binary and hex, since it is an encoding format which can convert a normal string/number into a series of encoded characters. It is different from binary and hexadecimal in the base that it uses. Base64 uses base 64 (0-63), while binary uses base 2 (0-1) and hexadecimal uses base 16 (0-9 and A-F). . Translate first 3 letters of your name to Base64.: Var --&gt; VmFy | . numpy . Numpy is described as &quot;The fundamental package for scientific computing with Python&quot;. In the Image Lab, a Numpy array is created from the image data in order to simplify access and change to the RGB values of the pixels, converting pixels to grey scale. . io, BytesIO . Input and Output (I/O) is a fundamental of all Computer Programming. Input/output (I/O) buffering is a technique used to optimize I/O operations. In large quantities of data, how many frames of input the server currently has queued is the buffer. In this example, there is a very large picture that lags. . Where have you been a consumer of buffering?:I have been a consumer of buffering when watching movies/TV shows online on sites such as 123movies, Soap2Day, etc.- From your consumer experience, what effects have you experienced from buffering? From buffering, the video/movie file often lags very consistently, and some parts of the video are simply unable to load, despite refreshing the page. This creates a bad movie watching experience, as parts of the movie are simply cut out. | How do these effects apply to images?: Buffering can be used to add certain edits to the features of images, such as greyscale, redscale, etc. | . Data Structures, Imperative Programming Style, and working with Images . Introduction to creating meta data and manipulating images. Look at each procedure and explain the the purpose and results of this program. Add any insights or challenges as you explored this program. . Does this code seem like a series of steps are being performed?:Yes, it seems like a series of steps are being performed to manipulate the images.- Describe Grey Scale algorithm in English or Pseudo code?: First, the series of images are prepared and added to the &quot;images&quot; dictionary. Then, to get the greyscale of the image, the average of all the pixels are taken, and the new image is loaded based on the averages. | Describe scale image? What is before and after on pixels in three images?: Scale image is a feature allowing for the images to get scaled up or down in size. Before scaling, the pixels are of a larger size, but after the averages of all the pixels are taken, they are compressed down to a smaller size. | Is scale image a type of compression? If so, line it up with College Board terms described?: Yes, scaling an image is a type of compression: lossy compression. This is where the image file size is reduced, but the image quality is also deteriorated. | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images # Large image scaled to baseWidth of 320 def scale_image(img): baseWidth = 320 scalePercent = (baseWidth/float(img.size[0])) scaleHeight = int((float(img.size[1])*float(scalePercent))) scale = (baseWidth, scaleHeight) return img.resize(scale) # PIL image converted to base64 def image_to_base64(img, format): with BytesIO() as buffer: img.save(buffer, format) return base64.b64encode(buffer.getvalue()).decode() # Set Properties of Image, Scale, and convert to Base64 def image_management(image): # path of static images is defaulted # Image open return PIL image object img = pilImage.open(image[&#39;filename&#39;]) # Python Image Library operations image[&#39;format&#39;] = img.format image[&#39;mode&#39;] = img.mode image[&#39;size&#39;] = img.size # Scale the Image img = scale_image(img) image[&#39;pil&#39;] = img image[&#39;scaled_size&#39;] = img.size # Scaled HTML image[&#39;html&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(image[&#39;pil&#39;], image[&#39;format&#39;]) # Create Grey Scale Base64 representation of Image def image_management_add_html_grey(image): # Image open return PIL image object img = image[&#39;pil&#39;] format = image[&#39;format&#39;] img_data = img.getdata() # Reference https://www.geeksforgeeks.org/python-pil-image-getdata/ image[&#39;data&#39;] = np.array(img_data) # PIL image to numpy array image[&#39;gray_data&#39;] = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in image[&#39;data&#39;]: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: image[&#39;gray_data&#39;].append((average, average, average, pixel[3])) # PNG format else: image[&#39;gray_data&#39;].append((average, average, average)) # end for loop for pixels img.putdata(image[&#39;gray_data&#39;]) image[&#39;html_grey&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(img, format) # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: # Use numpy to concatenate two arrays images = image_data() # Display meta data, scaled view, and grey scale for each image for image in images: image_management(image) print(&quot;- meta data --&quot;) print(image[&#39;label&#39;]) print(image[&#39;source&#39;]) print(image[&#39;format&#39;]) print(image[&#39;mode&#39;]) print(&quot;Original size: &quot;, image[&#39;size&#39;]) print(&quot;Scaled size: &quot;, image[&#39;scaled_size&#39;]) print(&quot;-- original image --&quot;) display(HTML(image[&#39;html&#39;])) print(&quot; grey image -&quot;) image_management_add_html_grey(image) display(HTML(image[&#39;html_grey&#39;])) print() . - meta data -- Green Square Internet PNG RGBA Original size: (16, 16) Scaled size: (320, 320) -- original image -- . grey image - . - meta data -- Clouds Impression Peter Carolin PNG RGBA Original size: (320, 234) Scaled size: (320, 234) -- original image -- . grey image - . - meta data -- Lassen Volcano Peter Carolin JPEG RGB Original size: (2792, 2094) Scaled size: (320, 240) -- original image -- . grey image - . . Data Structures and OOP . Most data structures classes require Object Oriented Programming (OOP). Since this class is lined up with a College Course, OOP will be talked about often. Functionality in remainder of this Blog is the same as the prior implementation. Highlight some of the key difference you see between imperative and oop styles. . Read imperative and object-oriented programming on Wikipedia | Consider how data is organized in two examples, in relations to procedures | Look at Parameters in Imperative and Self in OOP | . Additionally, review all the imports in these three demos. Create a definition of their purpose, specifically these ... . PIL:Stands for Python Imaging Library, and it adds many capabilities for processing different types of images. It is an open-source library, and it is often used to manipulate imported image properties.- numpy: This is another type of library that allows for the processing of mathematical functions in programs. | base64: This is an encoding algorithm representing binary data as text. | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np class Image_Data: def __init__(self, source, label, file, path, baseWidth=320): self._source = source # variables with self prefix become part of the object, self._label = label self._file = file self._filename = path / file # file with path self._baseWidth = baseWidth # Open image and scale to needs self._img = pilImage.open(self._filename) self._format = self._img.format self._mode = self._img.mode self._originalSize = self.img.size self.scale_image() self._html = self.image_to_html(self._img) self._html_grey = self.image_to_html_grey() @property def source(self): return self._source @property def label(self): return self._label @property def file(self): return self._file @property def filename(self): return self._filename @property def img(self): return self._img @property def format(self): return self._format @property def mode(self): return self._mode @property def originalSize(self): return self._originalSize @property def size(self): return self._img.size @property def html(self): return self._html @property def html_grey(self): return self._html_grey # Large image scaled to baseWidth of 320 def scale_image(self): scalePercent = (self._baseWidth/float(self._img.size[0])) scaleHeight = int((float(self._img.size[1])*float(scalePercent))) scale = (self._baseWidth, scaleHeight) self._img = self._img.resize(scale) # PIL image converted to base64 def image_to_html(self, img): with BytesIO() as buffer: img.save(buffer, self._format) return &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % base64.b64encode(buffer.getvalue()).decode() # Create Grey Scale Base64 representation of Image def image_to_html_grey(self): img_grey = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array grey_data = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in numpy: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: grey_data.append((average, average, average, pixel[3])) # PNG format else: grey_data.append((average, average, average)) # end for loop for pixels img_grey.putdata(grey_data) return self.image_to_html(img_grey) # prepares a series of images, provides expectation for required contents def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] return path, images # turns data into objects def image_objects(): id_Objects = [] path, images = image_data() for image in images: id_Objects.append(Image_Data(source=image[&#39;source&#39;], label=image[&#39;label&#39;], file=image[&#39;file&#39;], path=path, )) return id_Objects # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: for ido in image_objects(): # ido is an Imaged Data Object print(&quot;- meta data --&quot;) print(ido.label) print(ido.source) print(ido.file) print(ido.format) print(ido.mode) print(&quot;Original size: &quot;, ido.originalSize) print(&quot;Scaled size: &quot;, ido.size) print(&quot;-- scaled image --&quot;) display(HTML(ido.html)) print(&quot; grey image &quot;) display(HTML(ido.html_grey)) print() . - meta data -- Green Square Internet green-square-16.png PNG RGBA Original size: (16, 16) Scaled size: (320, 320) -- scaled image -- . grey image . - meta data -- Clouds Impression Peter Carolin clouds-impression.png PNG RGBA Original size: (320, 234) Scaled size: (320, 234) -- scaled image -- . grey image . - meta data -- Lassen Volcano Peter Carolin lassen-volcano.jpg JPEG RGB Original size: (2792, 2094) Scaled size: (320, 240) -- scaled image -- . grey image . . Hacks . Early Seed award . Add this Blog to you own Blogging site. | In the Blog add a Happy Face image. | Have Happy Face Image open when Tech Talk starts, running on localhost. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . In the Blog add notes and observations on each code cell that request an answer. | In blog add College Board practice problems for 2.3 | Choose 2 images, one that will more likely result in lossy data compression and one that is more likely to result in lossless data compression. Explain. | . Project Addition . If your project has images in it, try to implement an image change that has a purpose. (Ex. An item that has been sold out could become gray scale) | . Pick a programming paradigm and solve some of the following ... . Numpy, manipulating pixels. As opposed to Grey Scale treatment, pick a couple of other types like red scale, green scale, or blue scale. We want you to be manipulating pixels in the image. | Binary and Hexadecimal reports. Convert and produce pixels in binary and Hexadecimal and display. | Compression and Sizing of images. Look for insights into compression Lossy and Lossless. Look at PIL library and see if there are other things that can be done. | There are many effects you can do as well with PIL. Blur the image or write Meta Data on screen, aka Title, Author and Image size. | . . PROGRAMMING PARADIGMS . MANIPULATING PIXELS HACKS: . import numpy as np from PIL import Image # Load the image image = Image.open(&#39;images/baltimore.jpg&#39;) image.info[&#39;Title&#39;] = &#39;Masterpiece in the Air&#39; image.info[&#39;Author&#39;] = &#39;Vardaan Sinha&#39; image.info[&#39;Year&#39;] = &#39;2018&#39; img_array = np.asarray(image) binary_pixels = np.unpackbits(img_array, axis=-1) hex_pixels = np.apply_along_axis(lambda x: hex(int(&#39;&#39;.join(map(str, x)), 2))[2:].zfill(2), -1, binary_pixels) red_img = np.copy(img_array) red_img[:, :, 1] = 0 red_img[:, :, 2] = 0 red_image = Image.fromarray(red_img) red_image.save(&#39;images/baltimore.jpg&#39;) resized_image = red_image.resize((red_image.width // 4, red_image.height // 4)) print(image.info) resized_image.show() . {&#39;jfif&#39;: 257, &#39;jfif_version&#39;: (1, 1), &#39;jfif_unit&#39;: 0, &#39;jfif_density&#39;: (1, 1), &#39;Title&#39;: &#39;Masterpiece in the Air&#39;, &#39;Author&#39;: &#39;Vardaan Sinha&#39;, &#39;Year&#39;: &#39;2018&#39;} . PARADIGM EXPLANATION: . In the above hacks, I attempted to utilize redscale (a different byte manipulation method), printing metadata (important information about the image), and using scaling methods to change the size of the image. I utilized an image that I took (baltimore.jpg), and created red_img to set both the green and blue channels to be 0 (in &quot;RGB&quot;), so that only the red was captured, therefore making it redscale. I also added three pieces of image information (metadata) that were printed. This included the name of the photograph, the photographer, and the year that the image was taken in. The additional metadata citing &quot;jfif&quot; is for when a jpg/jpeg file is compressed. . TWO IMAGES REPRESENTING LOSSY AND LOSSLESS COMPRESSION: . The lassen-volcano.jpg image will represent lossy compression. This is because if the image file is compressed, it will definitely have a drastically reduced file size, but the image quality will also go down, due to the very fine details in the image, and the variety of colors. It will likely become more fuzzy. On the other hand, to represent lossless compression, the green-square-16.png image can be used as an example. That image has one sole color, so the RGB pixel values are going to be the same every time (barring any image manipulation method purposely utilized), and the quality of the image will not deteriorate, while the image file size will still decrease. . CB 2.2 PRACTICE QUIZZES: . . This was the only one with 2.2 as the listed topic. .",
            "url": "https://stg-7.github.io/FastPages/2023/03/11/ap22.html",
            "relUrl": "/2023/03/11/ap22.html",
            "date": " • Mar 11, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Unit 2.3 Extracting Information from Data, Pandas",
            "content": "Files To Get . Save this file to your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/2023-03-09-AP-unit2-3.ipynb . Save these files into a subfolder named files in your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/data.csv . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/grade.json . Save this image into a subfolder named images in your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/images/table_dataframe.png . Pandas and DataFrames . In this lesson we will be exploring data analysis using Pandas. . College Board talks about ideas like Tools. &quot;the ability to process data depends on users capabilities and their tools&quot; | Combining Data. &quot;combine county data sets&quot; | Status on Data&quot;determining the artist with the greatest attendance during a particular month&quot; | Data poses challenge. &quot;the need to clean data&quot;, &quot;incomplete data&quot; | . | . From Pandas Overview -- When working with tabular data, such as data stored in spreadsheets or databases, pandas is the right tool for you. pandas will help you to explore, clean, and process your data. In pandas, a data table is called a DataFrame. | . . &#39;&#39;&#39;Pandas is used to gather data sets through its DataFrames implementation&#39;&#39;&#39; import pandas as pd . Cleaning Data . When looking at a data set, check to see what data needs to be cleaned. Examples include: . Missing Data Points | Invalid Data | Inaccurate Data | . Run the following code to see what needs to be cleaned . df = pd.read_json(&#39;files/grade.json&#39;) print(df) # What part of the data set needs to be cleaned? # From PBL learning, what is a good time to clean data? Hint, remember Garbage in, Garbage out? . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 2 578 12 2.78 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 6 145 12 2.95 7 167 10 3.90 8 235 9th Grade 3.15 9 nil 9 2.80 10 469 11 3.45 11 456 10 2.75 . Extracting Info . Take a look at some features that the Pandas library has that extracts info from the dataset . DataFrame Extract Column . print(df[[&#39;GPA&#39;]]) print() #try two columns and remove the index from print statement print(df[[&#39;Student ID&#39;,&#39;GPA&#39;]].to_string(index=False)) . GPA 0 3.57 1 4.00 2 2.78 3 3.45 4 4.75 5 3.33 6 2.95 7 3.90 8 3.15 9 2.80 10 3.45 11 2.75 Student ID GPA 123 3.57 246 4.00 578 2.78 469 3.45 324 4.75 313 3.33 145 2.95 167 3.90 235 3.15 nil 2.80 469 3.45 456 2.75 . In the above code, only data from the column &quot;GPA&quot; is printed. Then, with the Student ID and GPA being printed, they attempt to print only two of the columns side by side. . DataFrame Sort . print(df.sort_values(by=[&#39;GPA&#39;])) print() #sort the values in reverse order print(df.sort_values(by=[&#39;GPA&#39;], ascending=True)) . Student ID Year in School GPA 11 456 10 2.75 2 578 12 2.78 9 nil 9 2.80 6 145 12 2.95 8 235 9th Grade 3.15 5 313 20 3.33 3 469 11 3.45 10 469 11 3.45 0 123 12 3.57 7 167 10 3.90 1 246 10 4.00 4 324 Junior 4.75 Student ID Year in School GPA 11 456 10 2.75 2 578 12 2.78 9 nil 9 2.80 6 145 12 2.95 8 235 9th Grade 3.15 5 313 20 3.33 3 469 11 3.45 10 469 11 3.45 0 123 12 3.57 7 167 10 3.90 1 246 10 4.00 4 324 Junior 4.75 . In this code snippet, all of the data is printed, but is sorted based on GPA. It is sorted from lowest GPA to highest GPA. . DataFrame Selection or Filter . print(df[df.GPA &gt; 3.00]) . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 7 167 10 3.90 8 235 9th Grade 3.15 10 469 11 3.45 . In this code snippet, only GPA values greater than a 3.0 are printed. . DataFrame Selection Max and Min . print(df[df.GPA == df.GPA.max()]) print() print(df[df.GPA == df.GPA.min()]) . Student ID Year in School GPA 4 324 Junior 4.75 Student ID Year in School GPA 11 456 10 2.75 . In this code snippet, the maximum and minimum GPA values are extracted from the data file and printed. . Create your own DataFrame . Using Pandas allows you to create your own DataFrame in Python. . Python Dictionary to Pandas DataFrame . import pandas as pd #the data can be stored as a python dictionary dict = { &quot;calories&quot;: [420, 380, 390], &quot;duration&quot;: [50, 40, 45] } #stores the data in a data frame print(&quot;-Dict_to_DF&quot;) df = pd.DataFrame(dict) print(df) print(&quot;-Dict_to_DF_labels--&quot;) #or with the index argument, you can label rows. df = pd.DataFrame(dict, index = [&quot;day1&quot;, &quot;day2&quot;, &quot;day3&quot;]) print(df) . -Dict_to_DF calories duration 0 420 50 1 380 40 2 390 45 -Dict_to_DF_labels-- calories duration day1 420 50 day2 380 40 day3 390 45 . In this code, a dictionary is created within the program itself. Then, the dictionary is sorted into readable data by creating columns with the data. Then, at the bottom labels are created with a starting index of &quot;day1&quot;, so that all of the rows can be labeled and corresponding to the right data. . Examine DataFrame Rows . print(&quot;-Examine Selected Rows&quot;) #use a list for multiple labels: print(df.loc[[&quot;day1&quot;, &quot;day3&quot;]]) #refer to the row index: print(&quot;--Examine Single Row--&quot;) print(df.loc[&quot;day1&quot;]) . -Examine Selected Rows calories duration day1 420 50 day3 390 45 --Examine Single Row-- calories 420 duration 50 Name: day1, dtype: int64 . Here, only certain rows of data are being pulled to analyze. In this case, it is only day1 and day3 statistics. . Pandas DataFrame Information . print(df.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 3 entries, day1 to day3 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 calories 3 non-null int64 1 duration 3 non-null int64 dtypes: int64(2) memory usage: 180.0+ bytes None . Here, there is information being printed about the dataset. It tells you the class, the index and how many entries are present in the data, the number of columns, the data type, and the corresponding memory usage of the dataset. . Example of larger data set . Pandas can read CSV and many other types of files, run the following code to see more features with a larger data set . import pandas as pd #read csv and sort &#39;Duration&#39; largest to smallest df = pd.read_csv(&#39;files/data.csv&#39;).sort_values(by=[&#39;Duration&#39;], ascending=False) print(&quot;--Duration Top 10&quot;) print(df.head(10)) print(&quot;--Duration Bottom 10&quot;) print(df.tail(10)) . --Duration Top 10 Duration Pulse Maxpulse Calories 69 300 108 143 1500.2 79 270 100 131 1729.0 109 210 137 184 1860.4 60 210 108 160 1376.0 106 180 90 120 800.3 90 180 101 127 600.1 65 180 90 130 800.4 61 160 110 137 1034.4 62 160 109 135 853.0 67 150 107 130 816.0 --Duration Bottom 10 Duration Pulse Maxpulse Calories 68 20 106 136 110.4 100 20 95 112 77.7 89 20 83 107 50.3 135 20 136 156 189.0 94 20 150 171 127.4 95 20 151 168 229.4 139 20 141 162 222.4 64 20 110 130 131.4 112 15 124 139 124.2 93 15 80 100 50.5 . This is very similar to what was being done before, except this time, there is more data, and it is stored in a CSV file (Excel spreadsheet), and it is being pulled from there. Then, at the bottom, it is printing the top 10 and bottom 10 entries of duration levels. . APIs are a Source for Writing Programs with Data . 3rd Party APIs are a great source for creating Pandas Data Frames. . Data can be fetched and resulting json can be placed into a Data Frame | Observe output, this looks very similar to a Database | . &#39;&#39;&#39;Pandas can be used to analyze data&#39;&#39;&#39; import pandas as pd import requests def fetch(): &#39;&#39;&#39;Obtain data from an endpoint&#39;&#39;&#39; url = &quot;https://flask.nighthawkcodingsociety.com/api/covid/&quot; fetch = requests.get(url) json = fetch.json() # filter data for requirement df = pd.DataFrame(json[&#39;countries_stat&#39;]) # filter endpoint for country stats print(df.loc[0:5, &#39;country_name&#39;:&#39;deaths&#39;]) # show row 0 through 5 and columns country_name through deaths fetch() . country_name cases deaths 0 USA 82,649,779 1,018,316 1 India 43,057,545 522,193 2 Brazil 30,345,654 662,663 3 France 28,244,977 145,020 4 Germany 24,109,433 134,624 5 UK 21,933,206 173,352 . This is something very similar to the fetch that was utilized in our PBL projects. The data is being fetched from our own API from the Flask server, and it is being read in a readable JSON format. Then, based on the name of the table &quot;countries_stat&quot;, the data is sorted and put into columns. The bottom line prints the first 6 rows based on case and death statistics. . Hacks . Early Seed award . Add this Blog to you own Blogging site. | Have all lecture files saved to your files directory before Tech Talk starts. Have data.csv open in vscode. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | In blog add College Board practice problems for 2.3. | . The next 4 weeks, Teachers want you to improve your understanding of data. Look at the blog and others on Unit 2. Your intention is to find some things to differentiate your individual College Board project. . Create or Find your own dataset. The suggestion is to use a JSON file, integrating with your PBL project would be Amazing. . | When choosing a data set, think about the following... . Does it have a good sample size? | Is there bias in the data? | Does the data set need to be cleaned? | What is the purpose of the data set? | ... | . | Continue this Blog using Pandas extract info from that dataset (ex. max, min, mean, median, mode, etc.) . | . Hack Helpers . Here is how Mort started on this assignment by asking ChatGPT ... Regarding Python Pandas, what are some data sets that would be good for learning Pandas? . There are many data sets that are suitable for learning pandas, depending on your interests and the skills you want to develop. . Here are some suggestions... . Titanic Dataset:This is a classic dataset for data analysis and machine learning, and is often used as an introduction to pandas. The dataset contains information about passengers on the Titanic, including their demographics, ticket class, and survival status. | Iris Dataset: This is another classic dataset that is often used in machine learning and data analysis courses. The dataset contains information about iris flowers, including their sepal length, sepal width, petal length, and petal width. . | NBA Player Stats: This dataset contains information about NBA players, including their stats for various games, their positions, and their salaries. It is a great dataset for learning how to clean and manipulate data using pandas. . | Housing Prices Dataset: This dataset contains information about housing prices in a particular city or region, including variables such as the number of bedrooms, square footage, and price. It is a great dataset for learning how to manipulate and analyze numerical data using pandas. . | COVID-19 Dataset: This dataset contains information about the COVID-19 pandemic, including the number of cases, deaths, and recoveries in various countries and regions. It is a great dataset for learning how to manipulate and analyze time-series data using pandas. . | World Bank Dataset: This dataset contains information about economic indicators from various countries, such as GDP, population, and poverty rates. It is a great dataset for learning how to manipulate and analyze large datasets using pandas. . | . | . These are just a few examples of the many datasets that are suitable for learning pandas. The best dataset for you will depend on your interests and the specific skills you want to develop. . Follow up question, &quot;where can I find Titanic data set?&quot; . | The Titanic dataset is a popular dataset for data analysis and machine learning, and it can be found on various websites and data repositories. Here are a few places where you can find the Titanic dataset... . Kaggle:The Titanic dataset is available on Kaggle, which is a popular platform for data scientists and machine learning enthusiasts. You can download the dataset from the Kaggle website after creating an account. | UCI Machine Learning Repository: The Titanic dataset is also available on the UCI Machine Learning Repository, which is a collection of datasets that are commonly used for machine learning research. You can download the dataset from the UCI Machine Learning Repository website. . | Seaborn library: If you have the Seaborn library installed in your Python environment, you can load the Titanic dataset directly from the library using the following code: . import seaborn as sns titanic_data = sns.load_dataset(&#39;titanic&#39;) . | . | . Titanic Data . Look at a sample of data. . import seaborn as sns # Load the titanic dataset titanic_data = sns.load_dataset(&#39;titanic&#39;) print(&quot;Titanic Data&quot;) print(titanic_data.columns) # titanic data set print(titanic_data[[&#39;survived&#39;,&#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;class&#39;, &#39;fare&#39;, &#39;embark_town&#39;]]) # look at selected columns . Titanic Data Index([&#39;survived&#39;, &#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;embarked&#39;, &#39;class&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;deck&#39;, &#39;embark_town&#39;, &#39;alive&#39;, &#39;alone&#39;], dtype=&#39;object&#39;) survived pclass sex age sibsp parch class fare 0 0 3 male 22.0 1 0 Third 7.2500 1 1 1 female 38.0 1 0 First 71.2833 2 1 3 female 26.0 0 0 Third 7.9250 3 1 1 female 35.0 1 0 First 53.1000 4 0 3 male 35.0 0 0 Third 8.0500 .. ... ... ... ... ... ... ... ... 886 0 2 male 27.0 0 0 Second 13.0000 887 1 1 female 19.0 0 0 First 30.0000 888 0 3 female NaN 1 2 Third 23.4500 889 1 1 male 26.0 0 0 First 30.0000 890 0 3 male 32.0 0 0 Third 7.7500 embark_town 0 Southampton 1 Cherbourg 2 Southampton 3 Southampton 4 Southampton .. ... 886 Southampton 887 Southampton 888 Southampton 889 Cherbourg 890 Queenstown [891 rows x 9 columns] . Use Pandas to clean the data. Most analysis, like Machine Learning or even Pandas in general like data to be in standardized format. This is called &#39;Training&#39; or &#39;Cleaning&#39; data. . # Preprocess the data from sklearn.preprocessing import OneHotEncoder td = titanic_data td.drop([&#39;alive&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;class&#39;, &#39;embark_town&#39;, &#39;deck&#39;], axis=1, inplace=True) td.dropna(inplace=True) td[&#39;sex&#39;] = td[&#39;sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) td[&#39;alone&#39;] = td[&#39;alone&#39;].apply(lambda x: 1 if x == True else 0) # Encode categorical variables enc = OneHotEncoder(handle_unknown=&#39;ignore&#39;) enc.fit(td[[&#39;embarked&#39;]]) onehot = enc.transform(td[[&#39;embarked&#39;]]).toarray() cols = [&#39;embarked_&#39; + val for val in enc.categories_[0]] td[cols] = pd.DataFrame(onehot) td.drop([&#39;embarked&#39;], axis=1, inplace=True) td.dropna(inplace=True) print(td) . survived pclass sex age sibsp parch fare alone embarked_C 0 0 3 1 22.0 1 0 7.2500 0 0.0 1 1 1 0 38.0 1 0 71.2833 0 1.0 2 1 3 0 26.0 0 0 7.9250 1 0.0 3 1 1 0 35.0 1 0 53.1000 0 0.0 4 0 3 1 35.0 0 0 8.0500 1 0.0 .. ... ... ... ... ... ... ... ... ... 705 0 2 1 39.0 0 0 26.0000 1 0.0 706 1 2 0 45.0 0 0 13.5000 1 0.0 707 1 1 1 42.0 0 0 26.2875 1 0.0 708 1 1 0 22.0 0 0 151.5500 1 0.0 710 1 1 0 24.0 0 0 49.5042 1 1.0 embarked_Q embarked_S 0 0.0 1.0 1 0.0 0.0 2 0.0 1.0 3 0.0 1.0 4 0.0 1.0 .. ... ... 705 0.0 1.0 706 0.0 1.0 707 1.0 0.0 708 0.0 1.0 710 0.0 0.0 [564 rows x 11 columns] . The result of &#39;Training&#39; data is making it easier to analyze or make conclusions. In looking at the Titanic, as you clean you would probably want to make assumptions on likely chance of survival. . This would involve analyzing various factors (such as age, gender, class, etc.) that may have affected a person&#39;s chances of survival, and using that information to make predictions about whether an individual would have survived or not. . Data description:- Survival - Survival (0 = No; 1 = Yes). Not included in test.csv file. - Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) . Name - Name | Sex - Sex | Age - Age | Sibsp - Number of Siblings/Spouses Aboard | Parch - Number of Parents/Children Aboard | Ticket - Ticket Number | Fare - Passenger Fare | Cabin - Cabin | Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) | . | Perished Mean/Average . | . print(titanic_data.query(&quot;survived == 0&quot;).mean()) . survived 0.000000 pclass 2.464072 sex 0.844311 age 31.073353 sibsp 0.562874 parch 0.398204 fare 24.835902 alone 0.616766 embarked_C 0.185629 embarked_Q 0.038922 embarked_S 0.775449 dtype: float64 . Survived Mean/Average | . print(td.query(&quot;survived == 1&quot;).mean()) . survived 1.000000 pclass 1.878261 sex 0.326087 age 28.481522 sibsp 0.504348 parch 0.508696 fare 50.188806 alone 0.456522 embarked_C 0.152174 embarked_Q 0.034783 embarked_S 0.813043 dtype: float64 . Survived Max and Min Stats . print(td.query(&quot;survived == 1&quot;).max()) print(td.query(&quot;survived == 1&quot;).min()) . survived 1.0000 pclass 3.0000 sex 1.0000 age 80.0000 sibsp 4.0000 parch 5.0000 fare 512.3292 alone 1.0000 embarked_C 1.0000 embarked_Q 1.0000 embarked_S 1.0000 dtype: float64 survived 1.00 pclass 1.00 sex 0.00 age 0.75 sibsp 0.00 parch 0.00 fare 0.00 alone 0.00 embarked_C 0.00 embarked_Q 0.00 embarked_S 0.00 dtype: float64 . Machine Learning Visit Tutorials Point . Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. . Description from ChatGPT. The Titanic dataset is a popular dataset for data analysis and machine learning. In the context of machine learning, accuracy refers to the percentage of correctly classified instances in a set of predictions. In this case, the testing data is a subset of the original Titanic dataset that the decision tree model has not seen during training......After training the decision tree model on the training data, we can evaluate its performance on the testing data by making predictions on the testing data and comparing them to the actual outcomes. The accuracy of the decision tree classifier on the testing data tells us how well the model generalizes to new data that it hasn&#39;t seen before......For example, if the accuracy of the decision tree classifier on the testing data is 0.8 (or 80%), this means that 80% of the predictions made by the model on the testing data were correct....Chance of survival could be done using various machine learning techniques, including decision trees, logistic regression, or support vector machines, among others. . | Code Below prepares data for further analysis and provides an Accuracy. IMO, you would insert a new passenger and predict survival. Datasets could be used on various factors like prediction if a player will hit a Home Run, or a Stock will go up or down. . Decision Trees, prediction by a piecewise constant approximation. | Logistic Regression, the probabilities describing the possible outcomes. | . | . from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Split arrays or matrices into random train and test subsets. X = td.drop(&#39;survived&#39;, axis=1) y = td[&#39;survived&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a decision tree classifier dt = DecisionTreeClassifier() dt.fit(X_train, y_train) # Test the model y_pred = dt.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;DecisionTreeClassifier Accuracy:&#39;, accuracy) # Train a logistic regression model logreg = LogisticRegression() logreg.fit(X_train, y_train) # Test the model y_pred = logreg.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;LogisticRegression Accuracy:&#39;, accuracy) . DecisionTreeClassifier Accuracy: 0.7705882352941177 LogisticRegression Accuracy: 0.788235294117647 . /Users/johnmortensen/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . import pandas as pd df = pd.read_json(&#39;files/nfl.json&#39;) print(&quot;HIGHEST QUARTERBACK HEIGHT&quot;) print(df[df.Height == df.Height.max()]) print() print(&quot;SMALLEST QUARTERBACK HEIGHT&quot;) print(df[df.Height == df.Height.min()]) print() print(&quot;TOP 5 QUARTERBACK HEIGHTS&quot;) print(df.head(5)) print() print() print() print(&quot;THE AVERAGE QUARTERBACK HEIGHT IN INCHES IS...&quot;) print(df[&#39;Height&#39;].mean()) print() print() print() print(&quot;THE MEDIAN QUARTERBACK WEIGHT IN POUNDS IS...&quot;) print(df[&#39;Weight&#39;].median()) print() print() print() print(&quot;SOME STATISTICS ABOUT THE PROSPECTS QUALITIES:&quot;) print(df.describe()) . HIGHEST QUARTERBACK HEIGHT Height Weight QB Ranking 0 76 244 1 2 76 229 3 SMALLEST QUARTERBACK HEIGHT Height Weight QB Ranking 1 70 214 2 TOP 5 QUARTERBACK HEIGHTS Height Weight QB Ranking 0 76 244 1 1 70 214 2 2 76 229 3 3 75 217 4 4 75 231 5 THE AVERAGE QUARTERBACK HEIGHT IN INCHES IS... 73.5 THE MEDIAN QUARTERBACK WEIGHT IN POUNDS IS... 213.5 SOME STATISTICS ABOUT THE PROSPECTS QUALITIES: Height Weight QB Ranking count 12.000000 12.000000 12.000000 mean 73.500000 215.666667 6.500000 std 1.977142 14.278613 3.605551 min 70.000000 192.000000 1.000000 25% 72.000000 207.000000 3.750000 50% 73.500000 213.500000 6.500000 75% 75.000000 225.250000 9.250000 max 76.000000 244.000000 12.000000 . EXPLANATION: . In the above code cell, I attempted to utilize plenty of Pandas utilities. I created my own dataset instead of using one from online, and gathered data based on 2023 NFL Quarterback Prospects straight out of college. I got data on the top 12 quarterbacks from this years draft, and used multiple sources to fact check the data that I was inputting. In this case, the three columns in the data were their height, weight, and quarterback ranking based on multiple analysts&#39; picks. I sorted this data and used Panda to list the top 5 quarterback heights, the biggest and smallest quarterbacks, the average quarterback height, the median quarterback weight, and something unique that I did was that I used the descriptive statistics function unique to Panda to list all of the IQR ranges, max values, etc based on the created dataset. All of the data was stored in nfl.json, a readable file. . 2.3 CB QUIZ: . .",
            "url": "https://stg-7.github.io/FastPages/2023/03/09/AP-unit2-3.html",
            "relUrl": "/2023/03/09/AP-unit2-3.html",
            "date": " • Mar 9, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "MCQ Test Blog 3/7/2023 (Tri. 2 Final)",
            "content": "Score Overview . from PIL import Image huh = Image.open(&quot;../images/huh.png&quot;) display(huh) . Wrong Answers . from PIL import Image wrong1 = Image.open(&quot;../images/wrong1.png&quot;) wrong2 = Image.open(&quot;../images/wrong2.png&quot;) wrong3 = Image.open(&quot;../images/wrong3.png&quot;) display(wrong1) display(wrong2) display(wrong3) . Correcting the Wrong Answers . Question 44 . The Corect Answer is B. The first call to printArgs displays the number 1, followed by the string &quot;*&quot;, followed by the number 1. The second call to printArgs displays the number 2, followed by the string &quot;&quot;, followed by the number 2. . Question 45 . The Correct Answers are C and D. Procedural abstraction helps improve code readability and Procedures created to solve subproblems of a larger problem are called by name. . Question 49 . The Correct Answer is B. Simulations are limited by the model that is used. There may be many reasons for using a simplified model, including ease of implementation. .",
            "url": "https://stg-7.github.io/FastPages/final2.html",
            "relUrl": "/final2.html",
            "date": " • Mar 7, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "Unit 2.2 Data Compression, Images",
            "content": "Enumerate &quot;Data&quot; Big Idea from College Board . Some of the big ideas and vocab that you observe, talk about it with a partner ... . &quot;Data compression is the reduction of the number of bits needed to represent data&quot; | &quot;Data compression is used to save transmission time and storage space.&quot; | &quot;lossy data can reduce data but the original data is not recovered&quot; | &quot;lossless data lets you restore and recover&quot; | . The Image Lab Project contains a plethora of College Board Unit 2 data concepts. Working with Images provides many opportunities for compression and analyzing size. . Image Files and Size . Here are some Images Files. Download these files, load them into images directory under _notebooks in your Blog. - Clouds Impression . Lassen Volcano | Green Square | . Describe some of the meta data and considerations when managing Image files. Describe how these relate to Data Compression ... . File Type, PNG and JPG are two types used in this lab | Size, height and width, number of pixels | Visual perception, lossy compression | . Python Libraries and Concepts used for Jupyter and Files/Directories . Introduction to displaying images in Jupyter notebook . IPython . Support visualization of data in Jupyter notebooks. Visualization is specific to View, for the web visualization needs to be converted to HTML. . pathlib . File paths are different on Windows versus Mac and Linux. This can cause problems in a project as you work and deploy on different Operating Systems (OS&#39;s), pathlib is a solution to this problem. . What are commands you use in terminal to access files? | What are the command you use in Windows terminal to access files? | What are some of the major differences? | . Provide what you observed, struggled with, or leaned while playing with this code. . Why is path a big deal when working with images? | How does the meta data source and label relate to Unit 5 topics? | Look up IPython, describe why this is interesting in Jupyter Notebooks for both Pandas and Images? | . from IPython.display import Image, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images def image_display(images): for image in images: display(Image(filename=image[&#39;filename&#39;])) # Run this as standalone tester to see sample data printed in Jupyter terminal if __name__ == &quot;__main__&quot;: # print parameter supplied image green_square = image_data(images=[{&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}]) image_display(green_square) smiley = image_data(images=[{&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Smiley&quot;, &#39;file&#39;: &quot;smiley.png&quot;}]) image_display(smiley) # display default images from image_data() default_images = image_data() image_display(default_images) . Reading and Encoding Images (2 implementations follow) . PIL (Python Image Library) . Pillow or PIL provides the ability to work with images in Python. Geeks for Geeks shows some ideas on working with images. . base64 . Image formats (JPG, PNG) are often called *Binary File formats, it is difficult to pass these over HTTP. Thus, base64 converts binary encoded data (8-bit, ASCII/Unicode) into a text encoded scheme (24 bits, 6-bit Base64 digits). Thus base64 is used to transport and embed binary images into textual assets such as HTML and CSS.- How is Base64 similar or different to Binary and Hexadecimal? . Translate first 3 letters of your name to Base64. | . numpy . Numpy is described as &quot;The fundamental package for scientific computing with Python&quot;. In the Image Lab, a Numpy array is created from the image data in order to simplify access and change to the RGB values of the pixels, converting pixels to grey scale. . io, BytesIO . Input and Output (I/O) is a fundamental of all Computer Programming. Input/output (I/O) buffering is a technique used to optimize I/O operations. In large quantities of data, how many frames of input the server currently has queued is the buffer. In this example, there is a very large picture that lags. . Where have you been a consumer of buffering? | From your consumer experience, what effects have you experienced from buffering? | How do these effects apply to images? | . Data Structures, Imperative Programming Style, and working with Images . Introduction to creating meta data and manipulating images. Look at each procedure and explain the the purpose and results of this program. Add any insights or challenges as you explored this program. . Does this code seem like a series of steps are being performed? | Describe Grey Scale algorithm in English or Pseudo code? | Describe scale image? What is before and after on pixels in three images? | Is scale image a type of compression? If so, line it up with College Board terms described? | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np # prepares a series of images def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] for image in images: # File to open image[&#39;filename&#39;] = path / image[&#39;file&#39;] # file with path return images # Large image scaled to baseWidth of 320 def scale_image(img): baseWidth = 320 scalePercent = (baseWidth/float(img.size[0])) scaleHeight = int((float(img.size[1])*float(scalePercent))) scale = (baseWidth, scaleHeight) return img.resize(scale) # PIL image converted to base64 def image_to_base64(img, format): with BytesIO() as buffer: img.save(buffer, format) return base64.b64encode(buffer.getvalue()).decode() # Set Properties of Image, Scale, and convert to Base64 def image_management(image): # path of static images is defaulted # Image open return PIL image object img = pilImage.open(image[&#39;filename&#39;]) # Python Image Library operations image[&#39;format&#39;] = img.format image[&#39;mode&#39;] = img.mode image[&#39;size&#39;] = img.size # Scale the Image img = scale_image(img) image[&#39;pil&#39;] = img image[&#39;scaled_size&#39;] = img.size # Scaled HTML image[&#39;html&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(image[&#39;pil&#39;], image[&#39;format&#39;]) # Create Grey Scale Base64 representation of Image def image_management_add_html_grey(image): # Image open return PIL image object img = image[&#39;pil&#39;] format = image[&#39;format&#39;] img_data = img.getdata() # Reference https://www.geeksforgeeks.org/python-pil-image-getdata/ image[&#39;data&#39;] = np.array(img_data) # PIL image to numpy array image[&#39;gray_data&#39;] = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in image[&#39;data&#39;]: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: image[&#39;gray_data&#39;].append((average, average, average, pixel[3])) # PNG format else: image[&#39;gray_data&#39;].append((average, average, average)) # end for loop for pixels img.putdata(image[&#39;gray_data&#39;]) image[&#39;html_grey&#39;] = &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % image_to_base64(img, format) # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: # Use numpy to concatenate two arrays images = image_data() # Display meta data, scaled view, and grey scale for each image for image in images: image_management(image) print(&quot;- meta data --&quot;) print(image[&#39;label&#39;]) print(image[&#39;source&#39;]) print(image[&#39;format&#39;]) print(image[&#39;mode&#39;]) print(&quot;Original size: &quot;, image[&#39;size&#39;]) print(&quot;Scaled size: &quot;, image[&#39;scaled_size&#39;]) print(&quot;-- original image --&quot;) display(HTML(image[&#39;html&#39;])) print(&quot; grey image -&quot;) image_management_add_html_grey(image) display(HTML(image[&#39;html_grey&#39;])) print() . - meta data -- Green Square Internet PNG RGBA Original size: (16, 16) Scaled size: (320, 320) -- original image -- . grey image - . - meta data -- Clouds Impression Peter Carolin PNG RGBA Original size: (320, 234) Scaled size: (320, 234) -- original image -- . grey image - . - meta data -- Lassen Volcano Peter Carolin JPEG RGB Original size: (2792, 2094) Scaled size: (320, 240) -- original image -- . grey image - . . Data Structures and OOP . Most data structures classes require Object Oriented Programming (OOP). Since this class is lined up with a College Course, OOP will be talked about often. Functionality in remainder of this Blog is the same as the prior implementation. Highlight some of the key difference you see between imperative and oop styles. . Read imperative and object-oriented programming on Wikipedia | Consider how data is organized in two examples, in relations to procedures | Look at Parameters in Imperative and Self in OOP | . Additionally, review all the imports in these three demos. Create a definition of their purpose, specifically these ... . PIL | numpy | base64 | . from IPython.display import HTML, display from pathlib import Path # https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f from PIL import Image as pilImage # as pilImage is used to avoid conflicts from io import BytesIO import base64 import numpy as np class Image_Data: def __init__(self, source, label, file, path, baseWidth=320): self._source = source # variables with self prefix become part of the object, self._label = label self._file = file self._filename = path / file # file with path self._baseWidth = baseWidth # Open image and scale to needs self._img = pilImage.open(self._filename) self._format = self._img.format self._mode = self._img.mode self._originalSize = self.img.size self.scale_image() self._html = self.image_to_html(self._img) self._html_grey = self.image_to_html_grey() @property def source(self): return self._source @property def label(self): return self._label @property def file(self): return self._file @property def filename(self): return self._filename @property def img(self): return self._img @property def format(self): return self._format @property def mode(self): return self._mode @property def originalSize(self): return self._originalSize @property def size(self): return self._img.size @property def html(self): return self._html @property def html_grey(self): return self._html_grey # Large image scaled to baseWidth of 320 def scale_image(self): scalePercent = (self._baseWidth/float(self._img.size[0])) scaleHeight = int((float(self._img.size[1])*float(scalePercent))) scale = (self._baseWidth, scaleHeight) self._img = self._img.resize(scale) # PIL image converted to base64 def image_to_html(self, img): with BytesIO() as buffer: img.save(buffer, self._format) return &#39;&lt;img src=&quot;data:image/png;base64,%s&quot;&gt;&#39; % base64.b64encode(buffer.getvalue()).decode() # Create Grey Scale Base64 representation of Image def image_to_html_grey(self): img_grey = self._img numpy = np.array(self._img.getdata()) # PIL image to numpy array grey_data = [] # key/value for data converted to gray scale # &#39;data&#39; is a list of RGB data, the list is traversed and hex and binary lists are calculated and formatted for pixel in numpy: # create gray scale of image, ref: https://www.geeksforgeeks.org/convert-a-numpy-array-to-an-image/ average = (pixel[0] + pixel[1] + pixel[2]) // 3 # average pixel values and use // for integer division if len(pixel) &gt; 3: grey_data.append((average, average, average, pixel[3])) # PNG format else: grey_data.append((average, average, average)) # end for loop for pixels img_grey.putdata(grey_data) return self.image_to_html(img_grey) # prepares a series of images, provides expectation for required contents def image_data(path=Path(&quot;images/&quot;), images=None): # path of static images is defaulted if images is None: # default image images = [ {&#39;source&#39;: &quot;Internet&quot;, &#39;label&#39;: &quot;Green Square&quot;, &#39;file&#39;: &quot;green-square-16.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Clouds Impression&quot;, &#39;file&#39;: &quot;clouds-impression.png&quot;}, {&#39;source&#39;: &quot;Peter Carolin&quot;, &#39;label&#39;: &quot;Lassen Volcano&quot;, &#39;file&#39;: &quot;lassen-volcano.jpg&quot;} ] return path, images # turns data into objects def image_objects(): id_Objects = [] path, images = image_data() for image in images: id_Objects.append(Image_Data(source=image[&#39;source&#39;], label=image[&#39;label&#39;], file=image[&#39;file&#39;], path=path, )) return id_Objects # Jupyter Notebook Visualization of Images if __name__ == &quot;__main__&quot;: for ido in image_objects(): # ido is an Imaged Data Object print(&quot;- meta data --&quot;) print(ido.label) print(ido.source) print(ido.file) print(ido.format) print(ido.mode) print(&quot;Original size: &quot;, ido.originalSize) print(&quot;Scaled size: &quot;, ido.size) print(&quot;-- scaled image --&quot;) display(HTML(ido.html)) print(&quot; grey image &quot;) display(HTML(ido.html_grey)) print() . - meta data -- Green Square Internet green-square-16.png PNG RGBA Original size: (16, 16) Scaled size: (320, 320) -- scaled image -- . grey image . - meta data -- Clouds Impression Peter Carolin clouds-impression.png PNG RGBA Original size: (320, 234) Scaled size: (320, 234) -- scaled image -- . grey image . - meta data -- Lassen Volcano Peter Carolin lassen-volcano.jpg JPEG RGB Original size: (2792, 2094) Scaled size: (320, 240) -- scaled image -- . grey image . . Hacks . Early Seed award . Add this Blog to you own Blogging site. | In the Blog add a Happy Face image. | Have Happy Face Image open when Tech Talk starts, running on localhost. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . In the Blog add notes and observations on each code cell that request an answer. | In blog add College Board practice problems for 2.3 | Choose 2 images, one that will more likely result in lossy data compression and one that is more likely to result in lossless data compression. Explain. | . Project Addition . If your project has images in it, try to implement an image change that has a purpose. (Ex. An item that has been sold out could become gray scale) | . Pick a programming paradigm and solve some of the following ... . Numpy, manipulating pixels. As opposed to Grey Scale treatment, pick a couple of other types like red scale, green scale, or blue scale. We want you to be manipulating pixels in the image. | Binary and Hexadecimal reports. Convert and produce pixels in binary and Hexadecimal and display. | Compression and Sizing of images. Look for insights into compression Lossy and Lossless. Look at PIL library and see if there are other things that can be done. | There are many effects you can do as well with PIL. Blur the image or write Meta Data on screen, aka Title, Author and Image size. | . .",
            "url": "https://stg-7.github.io/FastPages/2023/03/07/AP-unit2-2.html",
            "relUrl": "/2023/03/07/AP-unit2-2.html",
            "date": " • Mar 7, 2023"
        }
        
    
  
    
        ,"post4": {
            "title": "MCQ Test Blog 1/30/2023",
            "content": "MCQ Reflection and Blog . Overall score . . I got questions 17,29, and 50 wrong. | . ## Correcting Wrong Answers . ### Question 17 . . You had to choose one of two answers to this question. I chose the first option (A) correctly, but I chose Option B in error. When I reread the question, I saw that Option D contained a nested if-else statement that, unlike Option B, produced the same outcome as the question’s initial description (val1 &amp; not val2). . Question 29 . . The correct answer is A not B, because as it roates three times it will be facing downward. Furthermore, it will be in middle of the grid as the code goes to completion. . Question 50 . . The answer to the above-given question is option D. - Letter V . Further information and research on ASCII character encoding: . An international standard for encoding the Latin script was developed with the help of the American Standard Code for Information Interchange (ASCII). ASCII was first established in 1963 to enable the interpretation of data between computers. Symbols, numbers, lowercase and uppercase letters, and some instructions are represented. A radix-2 number system needs 7 bits since ASCII is encoded using 1s and 0s. Characters can be encoded using 2 to the power of 7 = 128 different number combinations with 7 bits. . Since ASCII only uses 7 bits to encode characters, using 8 bits in computing technology required the inclusion of additional bits. With this extra digit, extended ASCII up to 256 characters can be encoded. . As can be seen in the ASCII table above, ASCII encodes the specified 128 characters as 7-bit integers and was initially based on the (modern) English alphabet. One can print 95 of the encoded characters. These consist of the numbers 0 through 9, the letters lowercase a through z, uppercase a through z, and punctuation. The original ASCII specification also included 33 teletype machine-derived control codes that were not readable. Although the most of these are no longer in use, some are. Tab, line feed, and carriage return codes. . A lowercase letter, for instance, is represented by 1101001 in binary, 69 in hex (where I is the ninth character), and 105 in decimal in the ASCII encoding. American standard, however the cent () doesn’t have a code point in ASCII. Additionally, it does not accept proper nouns with diacritical marks, such as Beyoncé, or English phrases with diacritical marks, such resume or jalapeo. . Overall Reflection . Overall, I’m pleased with my grade because it demonstrates both my knowledge of the material covered in this class thus far and my ability to respond intelligently to multiple-choice questions of this type. The questions I misunderstood were also a teaching moment for me because they helped me understand how to approach issues of that nature going forward to avoid repeating my errors. Given that they nearly always result in real learning experiences for me, I enjoy having the chance to test myself on what we have studied so far and see how well I do. These kinds of exams let me know where my areas of strength and weakness lie and how I may strengthen those areas. I sincerely hope that when it comes time for this trimester’s multiple-choice final, I will demonstrate significant progress in how I respond to MCQ questions. .",
            "url": "https://stg-7.github.io/FastPages/markdown/2023/01/31/mcCB.html",
            "relUrl": "/markdown/2023/01/31/mcCB.html",
            "date": " • Jan 31, 2023"
        }
        
    
  
    
        ,"post5": {
            "title": "Physics Lab Report",
            "content": "Vardaan Sinha . Mr. Liao . AP Physics . 18 August 2022 . Lab #1: Measurements &amp; Graphical Analysis . Purpose Statement: The purpose of the lab is to find the relationship between the mass and radius of the disks and to calculate the uncertainty of the measured values through graphical analysis methods, such as linearization, where the graph shows a directly proportional relationship between the independent and dependent variables. . Derived Equation: . p = m/v → m = vp . | V = ah . | m = ( pi r^{2})hp . This equation shows that the mass of the disk is equal to the surface area of the disk (in other words, the area of a circle) multiplied by the height of the disk by the density of the disk. . | | Experiment Outline: . Measure the mass of the disk using the balance. Before using the balance, it must be zeroed, and then the disk can be placed on the balance. . | Measure the radius of the disk using the meter stick. The radius is measured by placing one end of the meter stick in the center of the circle, and the other end on the outside edge of the circle. . | Repeat Steps 1 &amp; 2 for each disk. . | Measure the thickness of one of the disks by folding the disk and measuring the height. Since the disk is folded to make it easier to find the height, divide the measurement appropriately to find the height if the disk was not folded. . | Graph the proportional mass vs radius2 relationship and determine the equation for the best-fit line. . | Precision: . The meter stick’s precision our group used was 1 millimeter (0.1 centimeters), since the meter stick used to measure the radius of the disks had markings down to the tenth of a centimeter. The measured radii of the disks in the data table is noted to the nearest hundredth of a centimeter, because of the 2/5/8 rounding rule, which added an extra digit of precision to the measurement. . | Data Table: . Radius2 (cm2) vs. Mass (grams) &amp; Radius (cm) . Radius (centimeters) Mass (grams)   Radius2 (centimeters2) . 5.32 cm | 0.37 g |   | 28.3024 cm2 | . 6.92 cm | 0.71 g |   | 47.8864 cm2 | . 4.12 cm | 0.25 g |   | 16.9744 cm2 | . 3.08 cm | 0.13 g |   | 9.4864 cm2 | . 2.28 cm | 0.08 g |   | 5.1984 cm2 | . . This is a nonlinear regression. . Equation: y = 0.0199151x2 - 0.0499334x + 0.095349 . . This is a linear regression. . Line of Best Fit Equation: y = 0.0146174x - 0.00729059 . Analysis Questions: . From the line of best fit, the independent variable represents the radius2 (centimeters2). This is because there is a linear relationship between the measured mass of each disk in grams and the corresponding squared radius values (in centimeters2) for each disk. . | The slope of the line of best fit has units of grams/cm2. In the original equation given in the lab, mass is equal to density multiplied by the disk’s surface area multiplied by the height of the disk. The surface area of the disk can be calculated through the formula for the area of the circle, or pi multiplied by the radius squared. Since the density, pi, and height values are all constant, they make up the coefficient for the slope of the line of best fit equation. Density is measured in grams/centimeter3, pi is a numerical constant, and height is measured in centimeters, and multiplying these values together gives units of grams/centimeter2. To verify that these are the correct units, the remaining part of the original equation is radius2, which has units of centimeters2, and multiplying the slope units (grams/centimeter2) by centimeters2 would produce a value in grams, which is the unit of measurement for m (mass). Since the y-value of the line of best fit equation has units of grams, the y-intercept (-0.00729059) must also be with units of grams, in order to produce the same unit on both sides of the equation: . In this case, g represents grams, and cm represents centimeters. . y = mx + b . | g = (g/cm2 * cm2) + g . | g = g + g . | g = g (verification) . | | | Yes, in ideal conditions, the “b” in the y = mx + b formula should be zero, as a disk with a radius of 0 centimeters would have a mass of 0 grams. . | Best-Fit Line Slope: 0.0146174 . The “thickness” of the cylinders is approximately 0.00125 centimeters. . | Density * ( pi) * Height = Slope . | Density * ( pi) * 0.00125 cm = 0.0146174 . | Density * 0.003925 cm = 0.0146174 . | Density = 3.72417834 ( approx) 3.72 g/cm3 . | The actual density of aluminum is 2.7 g/cm3. . | Percent Error: (3.72 g/cm3 - 2.7 g/cm3)/(2.7 g/cm3) x 100% = 37.778% ( approx) 38% . | | Our calculated density for the aluminum foil disk (3.72 g/cm3) was slightly larger than the actual calculated density of aluminum (2.7 g/cm3), as shown by our positive 38% error. Our number was likely larger than the actual value because of the inability to accurately measure the height of each disk, leading to an under measurement, creating a higher density value than expected. Furthermore, the measurement of the disk area may have been slightly inaccurate due to each disk not being perfectly circular. . | Synthesis Questions: . Disks with a greater thickness would correspond to an increase in the slope of the best-fit line. A greater thickness with an unchanged radius would result in a greater volume of the disk, leading to an increase in the slope of the line. The experimental density would stay the same, since the increased thickness would be reflected in the increased slope of the line. The slope is equal to the density multiplied by pi (constant) by the thickness of the disk, and with the increased thickness already accounted for in the increased slope, the density would remain the same as before. Disks with a greater thickness would have allowed for a more accurate measurement of the height, creating a more accurate density measurement. . | In this modified graph, the purple line (also circled) represents a scenario in which steel disks with the same thickness were used in the experiment. The slope for the line representing the steel disks is steeper than the line for the aluminum foil disks, since the density of steel (7.85 g/cm3) is much greater than that of aluminum (2.7 g/cm3), and the slope is directly proportional to the density. The purple line in the above graph has a slope of approximately 2.9 times greater than the best fit line showing the relationship between mass and radius2, as the density of steel is 2.9 times greater than that of aluminum. . | Work: . Slope: 122 kg/m2 . | Thickness: 0.5 cm . | Density * ( pi) * Thickness = Slope . | Density * ( pi) * 0.5 cm = 122 kg/m2 . | Density * 1.57 cm = 122 kg/m2 . | 122 kg/m2 x (1.0 m2/10,000 cm2) x (1000 g/1.0 kg) = 12.2 g/cm2 . | Density * 1.57 cm = 12.2 g/cm2 . | Density = 7.77 g/cm3 . | This is closest to the density of iron, which is 7.8 g/cm3. Therefore, the unknown material that these disks are made of is iron. . | | Multiple Choice Questions: . Mass = Density * Volume . Density * Area * Height = Mass . | Density * ( pi)r2 * Height = Mass . | C = 2( pi)r . | C2 = 2( pi)r2 . | C2/(2( pi)) = r2 . | Density * ( pi)(C2/(2( pi))2) * Height = Mass . | Mass = (Density * Height * 1/4( pi)) * C2 . | Slope = (Density * Height)/(4( pi)) . | Therefore, E is the correct option. . | | SKIP . | Volume of a Sphere = 4/3( pi)r3 (r = radius) . Mass = Density * Volume . | Mass = Density * 4/3( pi)r3 . | Mass = (Density * 4/3( pi)) x r3 . | Since density and 4/3( pi) are constants, the mass would need to be plotted versus the radius3, so the correct option is C. . | |",
            "url": "https://stg-7.github.io/FastPages/2022/08/28/Physics-Lab-Report.html",
            "relUrl": "/2022/08/28/Physics-Lab-Report.html",
            "date": " • Aug 28, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://stg-7.github.io/FastPages/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  
      ,"page0": {
          "title": "About Me",
          "content": "Hi! I’m Vardaan Sinha, and I’m a high school junior who loves math, computer science, cyber-security, writing, and solving 21st century problems. .",
          "url": "https://stg-7.github.io/FastPages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page1": {
          "title": "Computer Science Notes",
          "content": "Key Terms: . GitHub-Related Terms: . Repository: The location (for us, on Github) where we modify our fastpages site and collect all of our work for a particular project. . | Clone: “Cloning” a repository means to copy a repository onto your local system for modification. . | Commit: This means to save and commit changes in order for the changes to update on your site. . | VSCode Terms: . Source Control: This is the tab in VSCode where you stage, commit, and sync changes. . | Debug Console: This is where you can debug and work around any issues in your VSCode files. . | Application of these Terms: . Cloning a Repository through WSL: . (base) vardaan@LAPTOP-6CPSDHS5:~$ cd ./vscode # Changing the directory to your VSCode directory, which contains all of your cloned repositories. (base) vardaan@LAPTOP-6CPSDHS5:~/vscode$ git clone &quot;https://github.com/SamitPoojary/FASTPAGES.git&quot; # Cloning my partner Samit&#39;s FastPages so that I can track his changes. Cloning into &#39;FASTPAGES&#39;... remote: Enumerating objects: 722, done. remote: Counting objects: 100% (613/613), done. remote: Compressing objects: 100% (329/329), done. remote: Total 722 (delta 260), reused 479 (delta 191), pack-reused 109 Receiving objects: 100% (722/722), 9.46 MiB | 1.33 MiB/s, done. Resolving deltas: 100% (260/260), done. . Checking Status &amp; Making Commits through WSL: . (base) vardaan@LAPTOP-6CPSDHS5:~/vscode$ dir APCSP FASTPAGES FastPages fastpages # Showing the files that are in the &#39;vscode&#39; directory. (base) vardaan@LAPTOP-6CPSDHS5:~/vscode$ cd ./FastPages (base) vardaan@LAPTOP-6CPSDHS5:~/vscode/FastPages$ git status On branch master Your branch is up to date with &#39;origin/master&#39;. Changes not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: .vscode/settings.json modified: _pages/01_about.md modified: _pages/02_notes.md no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) # This command shows the status of the repository (any modified files or changes that have not been committed). (base) vardaan@LAPTOP-6CPSDHS5:~/vscode/FastPages$ git commit -a [master 746b938] committing large changes for bash committing large changes for bash 3 files changed, 21 insertions(+), 9 deletions(-) # This commits all the changes, and you can use VSCode to sync the changes. . Key Terms Continued (Trimester Cumulative) . Collaboration: One of the keys when working with a team on code. It is the art of communication that is essential to making functional code. . | Flask: Web Application Framework that is important to develop web applications. . | Jinja2: The Web Template Language that is compatible with Flask and Python. . | Bootstrap: An HTML/CSS template engine that helps to create more aesthetically appealing sites. . | Pseudo Code: This is essentially another term used to describe the comments that are made to describe each line or functionality of code. . | JavaScript: Another programming language that is essential, especially for frontend developers. . | Procedural Abstraction: The practice of promoting reuse instead of continuously coding the same thing again and again. . | Agile Methodology: The use of pair programming, test-driven development, stand-ups, planning sessions, and sprints. This also includes well defined team roles. . | AWS: Amazon Web Services is a tool that allows for the deployment of our projects. It is a large part of the backend of our project. . | Nginx: An open source software used for web serving, reverse proxy, caching, load balancing, media streaming, etc. . | Certbot: This is another open source software tool that allows for encrypted certifications for hosted sites. . | DNS: DNS, or Domain Name Services, allows for the assignment of a friendly name to a web server. . | API: Application Programming Interface, it is a way for multiple programs to communicate/call each other. . | API URL: The endpoint to which the API is directed. . | API Headers: The dictionary data structure used to send special messaging to the endpoint. . | Document Object Model: The Document Object Model, or DOM, is the interaction between HTML and JavaScript. . | HTTP/HTTPS: HTTP, or Hyper Text Transfer Protocol, is in charge of the communication between a web server and web browser. HTTPS, on the other hand, is in charge for more secure communication between a web server and a web browser. . | TCP/IP: Transmission Control Protocol, Internet Protocol. These messages are divided into small packets and sent between computers on the Internet. . | Network Layer: The wireless and physical layers that move bits across the Internet. . | Unit 2 Vocabulary: . Bits: A bit is a binary digit. It is the smallest size of data. . | Bytes: Bytes are groups of bits. . | Hexadecimal: Hex is base 16, and is used to show large values with a fewer amount of digits. . | Unsigned Integer: These are integers that don’t have any associated + or -. . | Signed Integer: These are integers with some sort of directionality (+ or -). . | Floating Point: These are decimals. . | Boolean: Boolean is a binary variable; can either be true or false. . | ASCII: This is a format of character encoding for text on the Internet. . | Unicode: A character encoding standard that assigns a unique number for each character in almost every language. . | RGB: This is used to represent colors on a system display. . | Lossy and lossless have not been discussed yet. . | Unit 3 Vocabulary: . Variables: Any sort of quantity which can be defined or measured. . | Data Types: These are types of data values. . | Assignment Operators: These are operators which assign values to variables. . | Lists: These are data types that have a number of ordered values. . | 2D Lists: This is a 2D data structure which can represent a matrix. . | Dictionaries: These are a method to store data values through a method known as keys and pairs. . | Class: A class contains all the details about a collection of variables and functions. . | Algorithms: Algorithms are processes which can help to solve certain problems through programming. . | Sequence: A sequence is an ordered set. . | Selection: This selects a certain part of a program to execute based on conditional statements. . | Iteration: This is where the same group of code is repeatedly executed. . | Expressions: These are a combination of operators and data values. . | Comparison Operators: This compares the two values and then tells the user whether the condition is met or not. . | Booleans Expressions and Selection/Iteration: These are a selection of code that allows for expressions to be iterated through or selected to see if a conditional is true/false (boolean). . | Truth Tables: This is a table to determine if a compound statement is true or false. . | Characters: These are singular keys that go into code. . | Strings: These are arrays of characters. . | Length: This is a function that counts the length of a string. . | Concatenation: Concatenation is the combination of two strings. . | Upper: This makes the string uppercase. . | Lower: This makes the string lowercase. . | Traversing Strings: These are strings that are processed one character at a time. . | If Statement: If a certain conditional is met, then the program will execute some block of code. . | Elif Statement: This checks if the above statement is true. . | Else Conditionals: Again, checks if something is true or not. . | Nested Selection Statements: This is where there is one selection within another. Multiple conditions can be tested here. . | For Loops: These are loops to iterate over a sequence. . | While Loops: While a certain conditional is true, a certain block of code is going to continue running. . | List Combining Loops: These are loops that can combine lists. . | Procedural Abstraction: This is when code sections can be generalized by creating parameters. This is useful, as code blocks can be utilized in multiple different program situations. . | Python def procedures: These are user-defined procedures. . | Parameters: These are placeholders for variables that a function utilizes. . | Return Values: These are the values that a function returns to the caller. . |",
          "url": "https://stg-7.github.io/FastPages/notes/",
          "relUrl": "/notes/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "JavaScript!",
          "content": "| HTML + JavaScript Table | Roster Cut JavaScript Algorithm | .",
          "url": "https://stg-7.github.io/FastPages/javascript/overview",
          "relUrl": "/javascript/overview",
          "date": ""
      }
      
  

  

  
      ,"page5": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://stg-7.github.io/FastPages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stg-7.github.io/FastPages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}