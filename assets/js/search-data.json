{
  
    
        "post0": {
            "title": "Unit 2.3 Extracting Information from Data, Pandas",
            "content": "Hacks . Early Seed award . Add this Blog to you own Blogging site. | Have all lecture files saved to your files directory before Tech Talk starts. Have data.csv open in vscode. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | In blog add College Board practice problems for 2.3. | . The next 4 weeks, Teachers want you to improve your understanding of data. Look at the blog and others on Unit 2. Your intention is to find some things to differentiate your individual College Board project. . Create or Find your own dataset. The suggestion is to use a JSON file, integrating with your PBL project would be Amazing. . | When choosing a data set, think about the following... . Does it have a good sample size? | Is there bias in the data? | Does the data set need to be cleaned? | What is the purpose of the data set? | ... | . | Continue this Blog using Pandas extract info from that dataset (ex. max, min, mean, median, mode, etc.) . | . Hack Helpers . Here is how Mort started on this assignment by asking ChatGPT ... Regarding Python Pandas, what are some data sets that would be good for learning Pandas? . There are many data sets that are suitable for learning pandas, depending on your interests and the skills you want to develop. . Here are some suggestions... . Titanic Dataset:This is a classic dataset for data analysis and machine learning, and is often used as an introduction to pandas. The dataset contains information about passengers on the Titanic, including their demographics, ticket class, and survival status. | Iris Dataset: This is another classic dataset that is often used in machine learning and data analysis courses. The dataset contains information about iris flowers, including their sepal length, sepal width, petal length, and petal width. . | NBA Player Stats: This dataset contains information about NBA players, including their stats for various games, their positions, and their salaries. It is a great dataset for learning how to clean and manipulate data using pandas. . | Housing Prices Dataset: This dataset contains information about housing prices in a particular city or region, including variables such as the number of bedrooms, square footage, and price. It is a great dataset for learning how to manipulate and analyze numerical data using pandas. . | COVID-19 Dataset: This dataset contains information about the COVID-19 pandemic, including the number of cases, deaths, and recoveries in various countries and regions. It is a great dataset for learning how to manipulate and analyze time-series data using pandas. . | World Bank Dataset: This dataset contains information about economic indicators from various countries, such as GDP, population, and poverty rates. It is a great dataset for learning how to manipulate and analyze large datasets using pandas. . | . | . These are just a few examples of the many datasets that are suitable for learning pandas. The best dataset for you will depend on your interests and the specific skills you want to develop. . Follow up question, &quot;where can I find Titanic data set?&quot; . | The Titanic dataset is a popular dataset for data analysis and machine learning, and it can be found on various websites and data repositories. Here are a few places where you can find the Titanic dataset... . Kaggle:The Titanic dataset is available on Kaggle, which is a popular platform for data scientists and machine learning enthusiasts. You can download the dataset from the Kaggle website after creating an account. | UCI Machine Learning Repository: The Titanic dataset is also available on the UCI Machine Learning Repository, which is a collection of datasets that are commonly used for machine learning research. You can download the dataset from the UCI Machine Learning Repository website. . | Seaborn library: If you have the Seaborn library installed in your Python environment, you can load the Titanic dataset directly from the library using the following code: . import seaborn as sns titanic_data = sns.load_dataset(&#39;titanic&#39;) . | . | . HACK #1 - AP PREP . from PIL import Image quizscore = Image.open(&quot;../images/cool.png&quot;) display(quizscore) . HACK #2 - Implementing into My PBL project . import pandas as pd df = pd.read_json(&#39;files/nfl.json&#39;) # Prinitng out objects in terms of Height (Ascending) print(df.sort_values(by=[&#39;Height&#39;], ascending=True)) print() print() print() # Prinitng out objects in terms of Weight (Ascending) print(df.sort_values(by=[&#39;Weight&#39;], ascending=True)) print() print() print() # Prinitng out objects in terms of QB Ranking (Ascending) print(df.sort_values(by=[&#39;QB Ranking&#39;], ascending=True)) print() print() print() . Height Weight QB Ranking 1 70 214 2 8 71 192 9 6 72 207 7 7 72 207 8 9 73 203 10 11 73 207 12 10 74 224 11 3 75 217 4 4 75 231 5 5 75 213 6 0 76 244 1 2 76 229 3 Height Weight QB Ranking 8 71 192 9 9 73 203 10 6 72 207 7 7 72 207 8 11 73 207 12 5 75 213 6 1 70 214 2 3 75 217 4 10 74 224 11 2 76 229 3 4 75 231 5 0 76 244 1 Height Weight QB Ranking 0 76 244 1 1 70 214 2 2 76 229 3 3 75 217 4 4 75 231 5 5 75 213 6 6 72 207 7 7 72 207 8 8 71 192 9 9 73 203 10 10 74 224 11 11 73 207 12 . print(df.sort_values(by=[&#39;Height&#39;], ascending=False)) print() print() print() # Prinitng out objects in terms of Weight (Ascending) print(df.sort_values(by=[&#39;Weight&#39;], ascending=False)) print() print() print() # Prinitng out objects in terms of QB Ranking (Ascending) print(df.sort_values(by=[&#39;QB Ranking&#39;], ascending=False)) print() print() print() . Height Weight QB Ranking 0 76 244 1 2 76 229 3 3 75 217 4 4 75 231 5 5 75 213 6 10 74 224 11 9 73 203 10 11 73 207 12 6 72 207 7 7 72 207 8 8 71 192 9 1 70 214 2 Height Weight QB Ranking 0 76 244 1 4 75 231 5 2 76 229 3 10 74 224 11 3 75 217 4 1 70 214 2 5 75 213 6 6 72 207 7 7 72 207 8 11 73 207 12 9 73 203 10 8 71 192 9 Height Weight QB Ranking 11 73 207 12 10 74 224 11 9 73 203 10 8 71 192 9 7 72 207 8 6 72 207 7 5 75 213 6 4 75 231 5 3 75 217 4 2 76 229 3 1 70 214 2 0 76 244 1 . print(&quot;Average Quarterback Height&quot;) print(df[&#39;Height&#39;].mean()) print() print(&quot;Median Quarterback Height&quot;) print(df[&#39;Height&#39;].median()) print() # Printing out Average and Median Quarterback Height print(&quot;Average Quarterback Weight&quot;) print(df[&#39;Weight&#39;].mean()) print() print(&quot;Median Quarterback Weight&quot;) print(df[&#39;Weight&#39;].median()) print() # Printing out Tallest and Shortest Quarterbacks print(&quot;Tallest Quarterback&quot;) print(df[df.Height == df.Height.max()]) print() print(&quot;Shortest Quarterback&quot;) print(df[df.Height == df.Height.min()]) . Average Quarterback Height 73.5 Median Quarterback Height 73.5 Average Quarterback Weight 215.66666666666666 Median Quarterback Weight 213.5 Tallest Quarterback Height Weight QB Ranking 0 76 244 1 2 76 229 3 Shortest Quarterback Height Weight QB Ranking 1 70 214 2 . EXPLANATION: . In the code box above, I tried using many Pandas helpers. Instead of using an online dataset, I created my own dataset and pulled the 2023 NFL Quarterback Prospects data directly from the university. I obtained information on the top 12 quarterbacks from this year&#39;s draft and used multiple sources to verify the information I entered. In this case, the three columns of data were their height, weight and quarterback rank based on multiple analysts. I sorted through this data and used Panda to list the top five quarterback heights, tallest and shortest quarterbacks, average quarterback height, average quarterback weight, and something unique that I did was use a descriptive statistics function unique to Panda to list. all IQR ranges, maximum values, etc. based on the generated dataset. All data was saved in nfl.json which is a readable file. .",
            "url": "https://stg-7.github.io/FastPages/2.3hacks.html",
            "relUrl": "/2.3hacks.html",
            "date": " • Mar 9, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "MCQ Test Blog 3/7/2023 (Tri. 2 Final)",
            "content": "Score Overview . from PIL import Image huh = Image.open(&quot;../images/huh.png&quot;) display(huh) . Wrong Answers . from PIL import Image wrong1 = Image.open(&quot;../images/wrong1.png&quot;) wrong2 = Image.open(&quot;../images/wrong2.png&quot;) wrong3 = Image.open(&quot;../images/wrong3.png&quot;) display(wrong1) display(wrong2) display(wrong3) . Correcting the Wrong Answers . Question 44 . The Corect Answer is B. The first call to printArgs displays the number 1, followed by the string &quot;*&quot;, followed by the number 1. The second call to printArgs displays the number 2, followed by the string &quot;&quot;, followed by the number 2. . Question 45 . The Correct Answers are C and D. Procedural abstraction helps improve code readability and Procedures created to solve subproblems of a larger problem are called by name. . Question 49 . The Correct Answer is B. Simulations are limited by the model that is used. There may be many reasons for using a simplified model, including ease of implementation. .",
            "url": "https://stg-7.github.io/FastPages/final2.html",
            "relUrl": "/final2.html",
            "date": " • Mar 7, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "2.2 Data Compression, Images HACKS",
            "content": "Hacks . Early Seed award . Add this Blog to you own Blogging site. | In the Blog add a Happy Face image. | Have Happy Face Image open when Tech Talk starts, running on localhost. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . In the Blog add notes and observations on each code cell that request an answer. | In blog add College Board practice problems for 2.3 | Choose 2 images, one that will more likely result in lossy data compression and one that is more likely to result in lossless data compression. Explain. | . Project Addition . If your project has images in it, try to implement an image change that has a purpose. (Ex. An item that has been sold out could become gray scale) | . Pick a programming paradigm and solve some of the following ... . Numpy, manipulating pixels. As opposed to Grey Scale treatment, pick a couple of other types like red scale, green scale, or blue scale. We want you to be manipulating pixels in the image. | Binary and Hexadecimal reports. Convert and produce pixels in binary and Hexadecimal and display. | Compression and Sizing of images. Look for insights into compression Lossy and Lossless. Look at PIL library and see if there are other things that can be done. | There are many effects you can do as well with PIL. Blur the image or write Meta Data on screen, aka Title, Author and Image size. | . HACK #1 - Happy Face Image . from PIL import Image smile = Image.open(&quot;../images/smiley.png&quot;) display(smile) . HACK #2 - AP PREP . Quiz Overview . from PIL import Image quizoverview = Image.open(&quot;../images/quiz1score.png&quot;) display(quizoverview) . Wong Answer + Solution . from PIL import Image wrong = Image.open(&quot;../images/quiz1wrong.png&quot;) display(wrong) . Solution: Lossless compression algorithms are guaranteed to be able to reconstruct the original data, while lossy compression algorithms are not. . HACK #3 - PROGRAMMING PARADIGMS . import numpy as np from PIL import Image # Load the image image = Image.open(&quot;../images/effeltower.jpg&quot;) image.info[&#39;Title&#39;] = &#39;Eiffel Tower&#39; image.info[&#39;Author&#39;] = &#39;Shaurya Goel&#39; image.info[&#39;Year&#39;] = &#39;2023&#39; img_array = np.asarray(image) binary_pixels = np.unpackbits(img_array, axis=-1) hex_pixels = np.apply_along_axis(lambda x: hex(int(&#39;&#39;.join(map(str, x)), 2))[2:].zfill(2), -1, binary_pixels) blue_image = np.copy(img_array) blue_image[:, :, 1] = 0 blue_image[:, :, 0] = 0 blue_image = Image.fromarray(blue_image) resized_image = blue_image.resize((blue_image.width // 4, blue_image.height // 4)) display(resized_image) . PARADIGM EXPLANATION: . In this HACK, I made an effort to use bluescale (a different byte manipulation technique), print metadata (vital information about the image), and alter the image&#39;s size using scaling techniques. I used a photograph I took, effeltower.jpg, and used the blue image program to convert it to a blue image by setting the green and blue channels to 0 (in &quot;RGB&quot;). Three printed pieces of image metadata were also added by me. This information contained the image&#39;s title, the photographer&#39;s name, and the year it was shot. When a jpg/jpeg file is compressed, additional metadata with the notation &quot;jfif&quot; is present. . TWO IMAGES REPRESENTING LOSSY AND LOSSLESS COMPRESSION: . Lossy = lassen-volcano.jpg | Lossless = green-square-16.png | . The lossy compression example will be the lassen-volcano.jpg image. This is owing to the fact that image file compression will undoubtedly result in a significantly smaller file size, but it will also result in a decrease in image quality because of the image&#39;s fine details and range of colors. It&#39;s probably going to get fuzzier.On the other hand, the green-square-16.png image can be used as an illustration of lossless compression. Because there is only one color in that image, the RGB pixel values will always be the same (without any intentional alteration of the image), the image quality will not change even though the image file size will reduce. .",
            "url": "https://stg-7.github.io/FastPages/2.2hacks.html",
            "relUrl": "/2.2hacks.html",
            "date": " • Mar 7, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "MCQ Test Blog 1/30/2023",
            "content": "MCQ Reflection and Blog . Overall score . . I got questions 17,29, and 50 wrong. | . ## Correcting Wrong Answers . ### Question 17 . . You had to choose one of two answers to this question. I chose the first option (A) correctly, but I chose Option B in error. When I reread the question, I saw that Option D contained a nested if-else statement that, unlike Option B, produced the same outcome as the question’s initial description (val1 &amp; not val2). . Question 29 . . The correct answer is A not B, because as it roates three times it will be facing downward. Furthermore, it will be in middle of the grid as the code goes to completion. . Question 50 . . The answer to the above-given question is option D. - Letter V . Further information and research on ASCII character encoding: . An international standard for encoding the Latin script was developed with the help of the American Standard Code for Information Interchange (ASCII). ASCII was first established in 1963 to enable the interpretation of data between computers. Symbols, numbers, lowercase and uppercase letters, and some instructions are represented. A radix-2 number system needs 7 bits since ASCII is encoded using 1s and 0s. Characters can be encoded using 2 to the power of 7 = 128 different number combinations with 7 bits. . Since ASCII only uses 7 bits to encode characters, using 8 bits in computing technology required the inclusion of additional bits. With this extra digit, extended ASCII up to 256 characters can be encoded. . As can be seen in the ASCII table above, ASCII encodes the specified 128 characters as 7-bit integers and was initially based on the (modern) English alphabet. One can print 95 of the encoded characters. These consist of the numbers 0 through 9, the letters lowercase a through z, uppercase a through z, and punctuation. The original ASCII specification also included 33 teletype machine-derived control codes that were not readable. Although the most of these are no longer in use, some are. Tab, line feed, and carriage return codes. . A lowercase letter, for instance, is represented by 1101001 in binary, 69 in hex (where I is the ninth character), and 105 in decimal in the ASCII encoding. American standard, however the cent () doesn’t have a code point in ASCII. Additionally, it does not accept proper nouns with diacritical marks, such as Beyoncé, or English phrases with diacritical marks, such resume or jalapeo. . Overall Reflection . Overall, I’m pleased with my grade because it demonstrates both my knowledge of the material covered in this class thus far and my ability to respond intelligently to multiple-choice questions of this type. The questions I misunderstood were also a teaching moment for me because they helped me understand how to approach issues of that nature going forward to avoid repeating my errors. Given that they nearly always result in real learning experiences for me, I enjoy having the chance to test myself on what we have studied so far and see how well I do. These kinds of exams let me know where my areas of strength and weakness lie and how I may strengthen those areas. I sincerely hope that when it comes time for this trimester’s multiple-choice final, I will demonstrate significant progress in how I respond to MCQ questions. .",
            "url": "https://stg-7.github.io/FastPages/markdown/2023/01/31/mcCB.html",
            "relUrl": "/markdown/2023/01/31/mcCB.html",
            "date": " • Jan 31, 2023"
        }
        
    
  
    
        ,"post4": {
            "title": "Physics Lab Report",
            "content": "Vardaan Sinha . Mr. Liao . AP Physics . 18 August 2022 . Lab #1: Measurements &amp; Graphical Analysis . Purpose Statement: The purpose of the lab is to find the relationship between the mass and radius of the disks and to calculate the uncertainty of the measured values through graphical analysis methods, such as linearization, where the graph shows a directly proportional relationship between the independent and dependent variables. . Derived Equation: . p = m/v → m = vp . | V = ah . | m = ( pi r^{2})hp . This equation shows that the mass of the disk is equal to the surface area of the disk (in other words, the area of a circle) multiplied by the height of the disk by the density of the disk. . | | Experiment Outline: . Measure the mass of the disk using the balance. Before using the balance, it must be zeroed, and then the disk can be placed on the balance. . | Measure the radius of the disk using the meter stick. The radius is measured by placing one end of the meter stick in the center of the circle, and the other end on the outside edge of the circle. . | Repeat Steps 1 &amp; 2 for each disk. . | Measure the thickness of one of the disks by folding the disk and measuring the height. Since the disk is folded to make it easier to find the height, divide the measurement appropriately to find the height if the disk was not folded. . | Graph the proportional mass vs radius2 relationship and determine the equation for the best-fit line. . | Precision: . The meter stick’s precision our group used was 1 millimeter (0.1 centimeters), since the meter stick used to measure the radius of the disks had markings down to the tenth of a centimeter. The measured radii of the disks in the data table is noted to the nearest hundredth of a centimeter, because of the 2/5/8 rounding rule, which added an extra digit of precision to the measurement. . | Data Table: . Radius2 (cm2) vs. Mass (grams) &amp; Radius (cm) . Radius (centimeters) Mass (grams)   Radius2 (centimeters2) . 5.32 cm | 0.37 g |   | 28.3024 cm2 | . 6.92 cm | 0.71 g |   | 47.8864 cm2 | . 4.12 cm | 0.25 g |   | 16.9744 cm2 | . 3.08 cm | 0.13 g |   | 9.4864 cm2 | . 2.28 cm | 0.08 g |   | 5.1984 cm2 | . . This is a nonlinear regression. . Equation: y = 0.0199151x2 - 0.0499334x + 0.095349 . . This is a linear regression. . Line of Best Fit Equation: y = 0.0146174x - 0.00729059 . Analysis Questions: . From the line of best fit, the independent variable represents the radius2 (centimeters2). This is because there is a linear relationship between the measured mass of each disk in grams and the corresponding squared radius values (in centimeters2) for each disk. . | The slope of the line of best fit has units of grams/cm2. In the original equation given in the lab, mass is equal to density multiplied by the disk’s surface area multiplied by the height of the disk. The surface area of the disk can be calculated through the formula for the area of the circle, or pi multiplied by the radius squared. Since the density, pi, and height values are all constant, they make up the coefficient for the slope of the line of best fit equation. Density is measured in grams/centimeter3, pi is a numerical constant, and height is measured in centimeters, and multiplying these values together gives units of grams/centimeter2. To verify that these are the correct units, the remaining part of the original equation is radius2, which has units of centimeters2, and multiplying the slope units (grams/centimeter2) by centimeters2 would produce a value in grams, which is the unit of measurement for m (mass). Since the y-value of the line of best fit equation has units of grams, the y-intercept (-0.00729059) must also be with units of grams, in order to produce the same unit on both sides of the equation: . In this case, g represents grams, and cm represents centimeters. . y = mx + b . | g = (g/cm2 * cm2) + g . | g = g + g . | g = g (verification) . | | | Yes, in ideal conditions, the “b” in the y = mx + b formula should be zero, as a disk with a radius of 0 centimeters would have a mass of 0 grams. . | Best-Fit Line Slope: 0.0146174 . The “thickness” of the cylinders is approximately 0.00125 centimeters. . | Density * ( pi) * Height = Slope . | Density * ( pi) * 0.00125 cm = 0.0146174 . | Density * 0.003925 cm = 0.0146174 . | Density = 3.72417834 ( approx) 3.72 g/cm3 . | The actual density of aluminum is 2.7 g/cm3. . | Percent Error: (3.72 g/cm3 - 2.7 g/cm3)/(2.7 g/cm3) x 100% = 37.778% ( approx) 38% . | | Our calculated density for the aluminum foil disk (3.72 g/cm3) was slightly larger than the actual calculated density of aluminum (2.7 g/cm3), as shown by our positive 38% error. Our number was likely larger than the actual value because of the inability to accurately measure the height of each disk, leading to an under measurement, creating a higher density value than expected. Furthermore, the measurement of the disk area may have been slightly inaccurate due to each disk not being perfectly circular. . | Synthesis Questions: . Disks with a greater thickness would correspond to an increase in the slope of the best-fit line. A greater thickness with an unchanged radius would result in a greater volume of the disk, leading to an increase in the slope of the line. The experimental density would stay the same, since the increased thickness would be reflected in the increased slope of the line. The slope is equal to the density multiplied by pi (constant) by the thickness of the disk, and with the increased thickness already accounted for in the increased slope, the density would remain the same as before. Disks with a greater thickness would have allowed for a more accurate measurement of the height, creating a more accurate density measurement. . | In this modified graph, the purple line (also circled) represents a scenario in which steel disks with the same thickness were used in the experiment. The slope for the line representing the steel disks is steeper than the line for the aluminum foil disks, since the density of steel (7.85 g/cm3) is much greater than that of aluminum (2.7 g/cm3), and the slope is directly proportional to the density. The purple line in the above graph has a slope of approximately 2.9 times greater than the best fit line showing the relationship between mass and radius2, as the density of steel is 2.9 times greater than that of aluminum. . | Work: . Slope: 122 kg/m2 . | Thickness: 0.5 cm . | Density * ( pi) * Thickness = Slope . | Density * ( pi) * 0.5 cm = 122 kg/m2 . | Density * 1.57 cm = 122 kg/m2 . | 122 kg/m2 x (1.0 m2/10,000 cm2) x (1000 g/1.0 kg) = 12.2 g/cm2 . | Density * 1.57 cm = 12.2 g/cm2 . | Density = 7.77 g/cm3 . | This is closest to the density of iron, which is 7.8 g/cm3. Therefore, the unknown material that these disks are made of is iron. . | | Multiple Choice Questions: . Mass = Density * Volume . Density * Area * Height = Mass . | Density * ( pi)r2 * Height = Mass . | C = 2( pi)r . | C2 = 2( pi)r2 . | C2/(2( pi)) = r2 . | Density * ( pi)(C2/(2( pi))2) * Height = Mass . | Mass = (Density * Height * 1/4( pi)) * C2 . | Slope = (Density * Height)/(4( pi)) . | Therefore, E is the correct option. . | | SKIP . | Volume of a Sphere = 4/3( pi)r3 (r = radius) . Mass = Density * Volume . | Mass = Density * 4/3( pi)r3 . | Mass = (Density * 4/3( pi)) x r3 . | Since density and 4/3( pi) are constants, the mass would need to be plotted versus the radius3, so the correct option is C. . | |",
            "url": "https://stg-7.github.io/FastPages/2022/08/28/Physics-Lab-Report.html",
            "relUrl": "/2022/08/28/Physics-Lab-Report.html",
            "date": " • Aug 28, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://stg-7.github.io/FastPages/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  
      ,"page0": {
          "title": "About Me",
          "content": "Hello, I’m Shaurya Goel. .",
          "url": "https://stg-7.github.io/FastPages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stg-7.github.io/FastPages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}