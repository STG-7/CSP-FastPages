{
  
    
        "post0": {
            "title": "Unit 2.3 Extracting Information from Data, Pandas",
            "content": "Files To Get . Save this file to your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/2023-03-09-AP-unit2-3.ipynb . Save these files into a subfolder named files in your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/data.csv . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/files/grade.json . Save this image into a subfolder named images in your _notebooks folder . wget https://raw.githubusercontent.com/nighthawkcoders/APCSP/master/_notebooks/images/table_dataframe.png . Pandas and DataFrames . In this lesson we will be exploring data analysis using Pandas. . College Board talks about ideas like Tools. &quot;the ability to process data depends on users capabilities and their tools&quot; | Combining Data. &quot;combine county data sets&quot; | Status on Data&quot;determining the artist with the greatest attendance during a particular month&quot; | Data poses challenge. &quot;the need to clean data&quot;, &quot;incomplete data&quot; | . | . From Pandas Overview -- When working with tabular data, such as data stored in spreadsheets or databases, pandas is the right tool for you. pandas will help you to explore, clean, and process your data. In pandas, a data table is called a DataFrame. | . . &#39;&#39;&#39;Pandas is used to gather data sets through its DataFrames implementation&#39;&#39;&#39; import pandas as pd . Cleaning Data . When looking at a data set, check to see what data needs to be cleaned. Examples include: . Missing Data Points | Invalid Data | Inaccurate Data | . Run the following code to see what needs to be cleaned . df = pd.read_json(&#39;files/grade.json&#39;) print(df) # What part of the data set needs to be cleaned? # From PBL learning, what is a good time to clean data? Hint, remember Garbage in, Garbage out? . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 2 578 12 2.78 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 6 145 12 2.95 7 167 10 3.90 8 235 9th Grade 3.15 9 nil 9 2.80 10 469 11 3.45 11 456 10 2.75 . Extracting Info . Take a look at some features that the Pandas library has that extracts info from the dataset . DataFrame Extract Column . print(df[[&#39;GPA&#39;]]) print() #try two columns and remove the index from print statement print(df[[&#39;Student ID&#39;,&#39;GPA&#39;]].to_string(index=False)) . GPA 0 3.57 1 4.00 2 2.78 3 3.45 4 4.75 5 3.33 6 2.95 7 3.90 8 3.15 9 2.80 10 3.45 11 2.75 Student ID GPA 123 3.57 246 4.00 578 2.78 469 3.45 324 4.75 313 3.33 145 2.95 167 3.90 235 3.15 nil 2.80 469 3.45 456 2.75 . In the above code, only data from the column &quot;GPA&quot; is printed. Then, with the Student ID and GPA being printed, they attempt to print only two of the columns side by side. . DataFrame Sort . print(df.sort_values(by=[&#39;GPA&#39;])) print() #sort the values in reverse order print(df.sort_values(by=[&#39;GPA&#39;], ascending=True)) . Student ID Year in School GPA 11 456 10 2.75 2 578 12 2.78 9 nil 9 2.80 6 145 12 2.95 8 235 9th Grade 3.15 5 313 20 3.33 3 469 11 3.45 10 469 11 3.45 0 123 12 3.57 7 167 10 3.90 1 246 10 4.00 4 324 Junior 4.75 Student ID Year in School GPA 11 456 10 2.75 2 578 12 2.78 9 nil 9 2.80 6 145 12 2.95 8 235 9th Grade 3.15 5 313 20 3.33 3 469 11 3.45 10 469 11 3.45 0 123 12 3.57 7 167 10 3.90 1 246 10 4.00 4 324 Junior 4.75 . In this code snippet, all of the data is printed, but is sorted based on GPA. It is sorted from lowest GPA to highest GPA. . DataFrame Selection or Filter . print(df[df.GPA &gt; 3.00]) . Student ID Year in School GPA 0 123 12 3.57 1 246 10 4.00 3 469 11 3.45 4 324 Junior 4.75 5 313 20 3.33 7 167 10 3.90 8 235 9th Grade 3.15 10 469 11 3.45 . In this code snippet, only GPA values greater than a 3.0 are printed. . DataFrame Selection Max and Min . print(df[df.GPA == df.GPA.max()]) print() print(df[df.GPA == df.GPA.min()]) . Student ID Year in School GPA 4 324 Junior 4.75 Student ID Year in School GPA 11 456 10 2.75 . In this code snippet, the maximum and minimum GPA values are extracted from the data file and printed. . Create your own DataFrame . Using Pandas allows you to create your own DataFrame in Python. . Python Dictionary to Pandas DataFrame . import pandas as pd #the data can be stored as a python dictionary dict = { &quot;calories&quot;: [420, 380, 390], &quot;duration&quot;: [50, 40, 45] } #stores the data in a data frame print(&quot;-Dict_to_DF&quot;) df = pd.DataFrame(dict) print(df) print(&quot;-Dict_to_DF_labels--&quot;) #or with the index argument, you can label rows. df = pd.DataFrame(dict, index = [&quot;day1&quot;, &quot;day2&quot;, &quot;day3&quot;]) print(df) . -Dict_to_DF calories duration 0 420 50 1 380 40 2 390 45 -Dict_to_DF_labels-- calories duration day1 420 50 day2 380 40 day3 390 45 . In this code, a dictionary is created within the program itself. Then, the dictionary is sorted into readable data by creating columns with the data. Then, at the bottom labels are created with a starting index of &quot;day1&quot;, so that all of the rows can be labeled and corresponding to the right data. . Examine DataFrame Rows . print(&quot;-Examine Selected Rows&quot;) #use a list for multiple labels: print(df.loc[[&quot;day1&quot;, &quot;day3&quot;]]) #refer to the row index: print(&quot;--Examine Single Row--&quot;) print(df.loc[&quot;day1&quot;]) . -Examine Selected Rows calories duration day1 420 50 day3 390 45 --Examine Single Row-- calories 420 duration 50 Name: day1, dtype: int64 . Here, only certain rows of data are being pulled to analyze. In this case, it is only day1 and day3 statistics. . Pandas DataFrame Information . print(df.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Index: 3 entries, day1 to day3 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 calories 3 non-null int64 1 duration 3 non-null int64 dtypes: int64(2) memory usage: 180.0+ bytes None . Here, there is information being printed about the dataset. It tells you the class, the index and how many entries are present in the data, the number of columns, the data type, and the corresponding memory usage of the dataset. . Example of larger data set . Pandas can read CSV and many other types of files, run the following code to see more features with a larger data set . import pandas as pd #read csv and sort &#39;Duration&#39; largest to smallest df = pd.read_csv(&#39;files/data.csv&#39;).sort_values(by=[&#39;Duration&#39;], ascending=False) print(&quot;--Duration Top 10&quot;) print(df.head(10)) print(&quot;--Duration Bottom 10&quot;) print(df.tail(10)) . --Duration Top 10 Duration Pulse Maxpulse Calories 69 300 108 143 1500.2 79 270 100 131 1729.0 109 210 137 184 1860.4 60 210 108 160 1376.0 106 180 90 120 800.3 90 180 101 127 600.1 65 180 90 130 800.4 61 160 110 137 1034.4 62 160 109 135 853.0 67 150 107 130 816.0 --Duration Bottom 10 Duration Pulse Maxpulse Calories 68 20 106 136 110.4 100 20 95 112 77.7 89 20 83 107 50.3 135 20 136 156 189.0 94 20 150 171 127.4 95 20 151 168 229.4 139 20 141 162 222.4 64 20 110 130 131.4 112 15 124 139 124.2 93 15 80 100 50.5 . This is very similar to what was being done before, except this time, there is more data, and it is stored in a CSV file (Excel spreadsheet), and it is being pulled from there. Then, at the bottom, it is printing the top 10 and bottom 10 entries of duration levels. . APIs are a Source for Writing Programs with Data . 3rd Party APIs are a great source for creating Pandas Data Frames. . Data can be fetched and resulting json can be placed into a Data Frame | Observe output, this looks very similar to a Database | . &#39;&#39;&#39;Pandas can be used to analyze data&#39;&#39;&#39; import pandas as pd import requests def fetch(): &#39;&#39;&#39;Obtain data from an endpoint&#39;&#39;&#39; url = &quot;https://flask.nighthawkcodingsociety.com/api/covid/&quot; fetch = requests.get(url) json = fetch.json() # filter data for requirement df = pd.DataFrame(json[&#39;countries_stat&#39;]) # filter endpoint for country stats print(df.loc[0:5, &#39;country_name&#39;:&#39;deaths&#39;]) # show row 0 through 5 and columns country_name through deaths fetch() . country_name cases deaths 0 USA 82,649,779 1,018,316 1 India 43,057,545 522,193 2 Brazil 30,345,654 662,663 3 France 28,244,977 145,020 4 Germany 24,109,433 134,624 5 UK 21,933,206 173,352 . This is something very similar to the fetch that was utilized in our PBL projects. The data is being fetched from our own API from the Flask server, and it is being read in a readable JSON format. Then, based on the name of the table &quot;countries_stat&quot;, the data is sorted and put into columns. The bottom line prints the first 6 rows based on case and death statistics. . Hacks . Early Seed award . Add this Blog to you own Blogging site. | Have all lecture files saved to your files directory before Tech Talk starts. Have data.csv open in vscode. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . Add this Blog to you own Blogging site. In the Blog add notes and observations on each code cell. | In blog add College Board practice problems for 2.3. | . The next 4 weeks, Teachers want you to improve your understanding of data. Look at the blog and others on Unit 2. Your intention is to find some things to differentiate your individual College Board project. . Create or Find your own dataset. The suggestion is to use a JSON file, integrating with your PBL project would be Amazing. . | When choosing a data set, think about the following... . Does it have a good sample size? | Is there bias in the data? | Does the data set need to be cleaned? | What is the purpose of the data set? | ... | . | Continue this Blog using Pandas extract info from that dataset (ex. max, min, mean, median, mode, etc.) . | . Hack Helpers . Here is how Mort started on this assignment by asking ChatGPT ... Regarding Python Pandas, what are some data sets that would be good for learning Pandas? . There are many data sets that are suitable for learning pandas, depending on your interests and the skills you want to develop. . Here are some suggestions... . Titanic Dataset:This is a classic dataset for data analysis and machine learning, and is often used as an introduction to pandas. The dataset contains information about passengers on the Titanic, including their demographics, ticket class, and survival status. | Iris Dataset: This is another classic dataset that is often used in machine learning and data analysis courses. The dataset contains information about iris flowers, including their sepal length, sepal width, petal length, and petal width. . | NBA Player Stats: This dataset contains information about NBA players, including their stats for various games, their positions, and their salaries. It is a great dataset for learning how to clean and manipulate data using pandas. . | Housing Prices Dataset: This dataset contains information about housing prices in a particular city or region, including variables such as the number of bedrooms, square footage, and price. It is a great dataset for learning how to manipulate and analyze numerical data using pandas. . | COVID-19 Dataset: This dataset contains information about the COVID-19 pandemic, including the number of cases, deaths, and recoveries in various countries and regions. It is a great dataset for learning how to manipulate and analyze time-series data using pandas. . | World Bank Dataset: This dataset contains information about economic indicators from various countries, such as GDP, population, and poverty rates. It is a great dataset for learning how to manipulate and analyze large datasets using pandas. . | . | . These are just a few examples of the many datasets that are suitable for learning pandas. The best dataset for you will depend on your interests and the specific skills you want to develop. . Follow up question, &quot;where can I find Titanic data set?&quot; . | The Titanic dataset is a popular dataset for data analysis and machine learning, and it can be found on various websites and data repositories. Here are a few places where you can find the Titanic dataset... . Kaggle:The Titanic dataset is available on Kaggle, which is a popular platform for data scientists and machine learning enthusiasts. You can download the dataset from the Kaggle website after creating an account. | UCI Machine Learning Repository: The Titanic dataset is also available on the UCI Machine Learning Repository, which is a collection of datasets that are commonly used for machine learning research. You can download the dataset from the UCI Machine Learning Repository website. . | Seaborn library: If you have the Seaborn library installed in your Python environment, you can load the Titanic dataset directly from the library using the following code: . import seaborn as sns titanic_data = sns.load_dataset(&#39;titanic&#39;) . | . | . Titanic Data . Look at a sample of data. . import seaborn as sns # Load the titanic dataset titanic_data = sns.load_dataset(&#39;titanic&#39;) print(&quot;Titanic Data&quot;) print(titanic_data.columns) # titanic data set print(titanic_data[[&#39;survived&#39;,&#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;class&#39;, &#39;fare&#39;, &#39;embark_town&#39;]]) # look at selected columns . Titanic Data Index([&#39;survived&#39;, &#39;pclass&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;sibsp&#39;, &#39;parch&#39;, &#39;fare&#39;, &#39;embarked&#39;, &#39;class&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;deck&#39;, &#39;embark_town&#39;, &#39;alive&#39;, &#39;alone&#39;], dtype=&#39;object&#39;) survived pclass sex age sibsp parch class fare 0 0 3 male 22.0 1 0 Third 7.2500 1 1 1 female 38.0 1 0 First 71.2833 2 1 3 female 26.0 0 0 Third 7.9250 3 1 1 female 35.0 1 0 First 53.1000 4 0 3 male 35.0 0 0 Third 8.0500 .. ... ... ... ... ... ... ... ... 886 0 2 male 27.0 0 0 Second 13.0000 887 1 1 female 19.0 0 0 First 30.0000 888 0 3 female NaN 1 2 Third 23.4500 889 1 1 male 26.0 0 0 First 30.0000 890 0 3 male 32.0 0 0 Third 7.7500 embark_town 0 Southampton 1 Cherbourg 2 Southampton 3 Southampton 4 Southampton .. ... 886 Southampton 887 Southampton 888 Southampton 889 Cherbourg 890 Queenstown [891 rows x 9 columns] . Use Pandas to clean the data. Most analysis, like Machine Learning or even Pandas in general like data to be in standardized format. This is called &#39;Training&#39; or &#39;Cleaning&#39; data. . # Preprocess the data from sklearn.preprocessing import OneHotEncoder td = titanic_data td.drop([&#39;alive&#39;, &#39;who&#39;, &#39;adult_male&#39;, &#39;class&#39;, &#39;embark_town&#39;, &#39;deck&#39;], axis=1, inplace=True) td.dropna(inplace=True) td[&#39;sex&#39;] = td[&#39;sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) td[&#39;alone&#39;] = td[&#39;alone&#39;].apply(lambda x: 1 if x == True else 0) # Encode categorical variables enc = OneHotEncoder(handle_unknown=&#39;ignore&#39;) enc.fit(td[[&#39;embarked&#39;]]) onehot = enc.transform(td[[&#39;embarked&#39;]]).toarray() cols = [&#39;embarked_&#39; + val for val in enc.categories_[0]] td[cols] = pd.DataFrame(onehot) td.drop([&#39;embarked&#39;], axis=1, inplace=True) td.dropna(inplace=True) print(td) . survived pclass sex age sibsp parch fare alone embarked_C 0 0 3 1 22.0 1 0 7.2500 0 0.0 1 1 1 0 38.0 1 0 71.2833 0 1.0 2 1 3 0 26.0 0 0 7.9250 1 0.0 3 1 1 0 35.0 1 0 53.1000 0 0.0 4 0 3 1 35.0 0 0 8.0500 1 0.0 .. ... ... ... ... ... ... ... ... ... 705 0 2 1 39.0 0 0 26.0000 1 0.0 706 1 2 0 45.0 0 0 13.5000 1 0.0 707 1 1 1 42.0 0 0 26.2875 1 0.0 708 1 1 0 22.0 0 0 151.5500 1 0.0 710 1 1 0 24.0 0 0 49.5042 1 1.0 embarked_Q embarked_S 0 0.0 1.0 1 0.0 0.0 2 0.0 1.0 3 0.0 1.0 4 0.0 1.0 .. ... ... 705 0.0 1.0 706 0.0 1.0 707 1.0 0.0 708 0.0 1.0 710 0.0 0.0 [564 rows x 11 columns] . The result of &#39;Training&#39; data is making it easier to analyze or make conclusions. In looking at the Titanic, as you clean you would probably want to make assumptions on likely chance of survival. . This would involve analyzing various factors (such as age, gender, class, etc.) that may have affected a person&#39;s chances of survival, and using that information to make predictions about whether an individual would have survived or not. . Data description:- Survival - Survival (0 = No; 1 = Yes). Not included in test.csv file. - Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) . Name - Name | Sex - Sex | Age - Age | Sibsp - Number of Siblings/Spouses Aboard | Parch - Number of Parents/Children Aboard | Ticket - Ticket Number | Fare - Passenger Fare | Cabin - Cabin | Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) | . | Perished Mean/Average . | . print(titanic_data.query(&quot;survived == 0&quot;).mean()) . survived 0.000000 pclass 2.464072 sex 0.844311 age 31.073353 sibsp 0.562874 parch 0.398204 fare 24.835902 alone 0.616766 embarked_C 0.185629 embarked_Q 0.038922 embarked_S 0.775449 dtype: float64 . Survived Mean/Average | . print(td.query(&quot;survived == 1&quot;).mean()) . survived 1.000000 pclass 1.878261 sex 0.326087 age 28.481522 sibsp 0.504348 parch 0.508696 fare 50.188806 alone 0.456522 embarked_C 0.152174 embarked_Q 0.034783 embarked_S 0.813043 dtype: float64 . Survived Max and Min Stats . print(td.query(&quot;survived == 1&quot;).max()) print(td.query(&quot;survived == 1&quot;).min()) . survived 1.0000 pclass 3.0000 sex 1.0000 age 80.0000 sibsp 4.0000 parch 5.0000 fare 512.3292 alone 1.0000 embarked_C 1.0000 embarked_Q 1.0000 embarked_S 1.0000 dtype: float64 survived 1.00 pclass 1.00 sex 0.00 age 0.75 sibsp 0.00 parch 0.00 fare 0.00 alone 0.00 embarked_C 0.00 embarked_Q 0.00 embarked_S 0.00 dtype: float64 . Machine Learning Visit Tutorials Point . Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. . Description from ChatGPT. The Titanic dataset is a popular dataset for data analysis and machine learning. In the context of machine learning, accuracy refers to the percentage of correctly classified instances in a set of predictions. In this case, the testing data is a subset of the original Titanic dataset that the decision tree model has not seen during training......After training the decision tree model on the training data, we can evaluate its performance on the testing data by making predictions on the testing data and comparing them to the actual outcomes. The accuracy of the decision tree classifier on the testing data tells us how well the model generalizes to new data that it hasn&#39;t seen before......For example, if the accuracy of the decision tree classifier on the testing data is 0.8 (or 80%), this means that 80% of the predictions made by the model on the testing data were correct....Chance of survival could be done using various machine learning techniques, including decision trees, logistic regression, or support vector machines, among others. . | Code Below prepares data for further analysis and provides an Accuracy. IMO, you would insert a new passenger and predict survival. Datasets could be used on various factors like prediction if a player will hit a Home Run, or a Stock will go up or down. . Decision Trees, prediction by a piecewise constant approximation. | Logistic Regression, the probabilities describing the possible outcomes. | . | . from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Split arrays or matrices into random train and test subsets. X = td.drop(&#39;survived&#39;, axis=1) y = td[&#39;survived&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train a decision tree classifier dt = DecisionTreeClassifier() dt.fit(X_train, y_train) # Test the model y_pred = dt.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;DecisionTreeClassifier Accuracy:&#39;, accuracy) # Train a logistic regression model logreg = LogisticRegression() logreg.fit(X_train, y_train) # Test the model y_pred = logreg.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(&#39;LogisticRegression Accuracy:&#39;, accuracy) . DecisionTreeClassifier Accuracy: 0.7705882352941177 LogisticRegression Accuracy: 0.788235294117647 . /Users/johnmortensen/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . import pandas as pd df = pd.read_json(&#39;files/nfl.json&#39;) print(&quot;HIGHEST QUARTERBACK HEIGHT&quot;) print(df[df.Height == df.Height.max()]) print() print(&quot;SMALLEST QUARTERBACK HEIGHT&quot;) print(df[df.Height == df.Height.min()]) print() print(&quot;TOP 5 QUARTERBACK HEIGHTS&quot;) print(df.head(5)) print() print() print() print(&quot;THE AVERAGE QUARTERBACK HEIGHT IN INCHES IS...&quot;) print(df[&#39;Height&#39;].mean()) print() print() print() print(&quot;THE MEDIAN QUARTERBACK WEIGHT IN POUNDS IS...&quot;) print(df[&#39;Weight&#39;].median()) print() print() print() print(&quot;SOME STATISTICS ABOUT THE PROSPECTS QUALITIES:&quot;) print(df.describe()) . HIGHEST QUARTERBACK HEIGHT Height Weight QB Ranking 0 76 244 1 2 76 229 3 SMALLEST QUARTERBACK HEIGHT Height Weight QB Ranking 1 70 214 2 TOP 5 QUARTERBACK HEIGHTS Height Weight QB Ranking 0 76 244 1 1 70 214 2 2 76 229 3 3 75 217 4 4 75 231 5 THE AVERAGE QUARTERBACK HEIGHT IN INCHES IS... 73.5 THE MEDIAN QUARTERBACK WEIGHT IN POUNDS IS... 213.5 SOME STATISTICS ABOUT THE PROSPECTS QUALITIES: Height Weight QB Ranking count 12.000000 12.000000 12.000000 mean 73.500000 215.666667 6.500000 std 1.977142 14.278613 3.605551 min 70.000000 192.000000 1.000000 25% 72.000000 207.000000 3.750000 50% 73.500000 213.500000 6.500000 75% 75.000000 225.250000 9.250000 max 76.000000 244.000000 12.000000 . EXPLANATION: . In the above code cell, I attempted to utilize plenty of Pandas utilities. I created my own dataset instead of using one from online, and gathered data based on 2023 NFL Quarterback Prospects straight out of college. I got data on the top 12 quarterbacks from this years draft, and used multiple sources to fact check the data that I was inputting. In this case, the three columns in the data were their height, weight, and quarterback ranking based on multiple analysts&#39; picks. I sorted this data and used Panda to list the top 5 quarterback heights, the biggest and smallest quarterbacks, the average quarterback height, the median quarterback weight, and something unique that I did was that I used the descriptive statistics function unique to Panda to list all of the IQR ranges, max values, etc based on the created dataset. All of the data was stored in nfl.json, a readable file. . 2.3 CB QUIZ: . .",
            "url": "https://stg-7.github.io/FastPages/2.3hacks.html",
            "relUrl": "/2.3hacks.html",
            "date": " • Mar 9, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "MCQ Test Blog 3/7/2023 (Tri. 2 Final)",
            "content": "Score Overview . from PIL import Image huh = Image.open(&quot;../images/huh.png&quot;) display(huh) . Wrong Answers . from PIL import Image wrong1 = Image.open(&quot;../images/wrong1.png&quot;) wrong2 = Image.open(&quot;../images/wrong2.png&quot;) wrong3 = Image.open(&quot;../images/wrong3.png&quot;) display(wrong1) display(wrong2) display(wrong3) . Correcting the Wrong Answers . Question 44 . The Corect Answer is B. The first call to printArgs displays the number 1, followed by the string &quot;*&quot;, followed by the number 1. The second call to printArgs displays the number 2, followed by the string &quot;&quot;, followed by the number 2. . Question 45 . The Correct Answers are C and D. Procedural abstraction helps improve code readability and Procedures created to solve subproblems of a larger problem are called by name. . Question 49 . The Correct Answer is B. Simulations are limited by the model that is used. There may be many reasons for using a simplified model, including ease of implementation. .",
            "url": "https://stg-7.github.io/FastPages/final2.html",
            "relUrl": "/final2.html",
            "date": " • Mar 7, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "Unit 2.2 Data Compression, Images",
            "content": "Hacks . Early Seed award . Add this Blog to you own Blogging site. | In the Blog add a Happy Face image. | Have Happy Face Image open when Tech Talk starts, running on localhost. Don&#39;t tell anyone. Show to Teacher. | . AP Prep . In the Blog add notes and observations on each code cell that request an answer. | In blog add College Board practice problems for 2.3 | Choose 2 images, one that will more likely result in lossy data compression and one that is more likely to result in lossless data compression. Explain. | . Project Addition . If your project has images in it, try to implement an image change that has a purpose. (Ex. An item that has been sold out could become gray scale) | . Pick a programming paradigm and solve some of the following ... . Numpy, manipulating pixels. As opposed to Grey Scale treatment, pick a couple of other types like red scale, green scale, or blue scale. We want you to be manipulating pixels in the image. | Binary and Hexadecimal reports. Convert and produce pixels in binary and Hexadecimal and display. | Compression and Sizing of images. Look for insights into compression Lossy and Lossless. Look at PIL library and see if there are other things that can be done. | There are many effects you can do as well with PIL. Blur the image or write Meta Data on screen, aka Title, Author and Image size. | . HACK #1 - Happy Face Image . from PIL import Image smile = Image.open(&quot;../images/smiley.png&quot;) display(smile) . HACK #2 - AP PREP . HACK #3 - PROJECT ADDITION . HACK #4 - PROGRAMMING PARADIGMS . import numpy as np from PIL import Image # Load the image image = Image.open(&quot;../images/effeltower.jpg&quot;) image.info[&#39;Title&#39;] = &#39;Eiffel Tower&#39; image.info[&#39;Author&#39;] = &#39;Shaurya GOel&#39; image.info[&#39;Year&#39;] = &#39;2023&#39; img_array = np.asarray(image) binary_pixels = np.unpackbits(img_array, axis=-1) hex_pixels = np.apply_along_axis(lambda x: hex(int(&#39;&#39;.join(map(str, x)), 2))[2:].zfill(2), -1, binary_pixels) red_img = np.copy(img_array) red_img[:, :, 1] = 0 red_img[:, :, 2] = 0 red_image = Image.fromarray(red_img) red_image.save(&#39;images/baltimore.jpg&#39;) resized_image = red_image.resize((red_image.width // 4, red_image.height // 4)) display(resized_image) . {&#39;jfif&#39;: 257, &#39;jfif_version&#39;: (1, 1), &#39;jfif_unit&#39;: 0, &#39;jfif_density&#39;: (1, 1), &#39;progressive&#39;: 1, &#39;progression&#39;: 1, &#39;icc_profile&#39;: b&#39; x00 x00 x0cHLino x02 x10 x00 x00mntrRGB XYZ x07 xce x00 x02 x00 t x00 x06 x001 x00 x00acspMSFT x00 x00 x00 x00IEC sRGB x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 xf6 xd6 x00 x01 x00 x00 x00 x00 xd3-HP x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x11cprt x00 x00 x01P x00 x00 x003desc x00 x00 x01 x84 x00 x00 x00lwtpt x00 x00 x01 xf0 x00 x00 x00 x14bkpt x00 x00 x02 x04 x00 x00 x00 x14rXYZ x00 x00 x02 x18 x00 x00 x00 x14gXYZ x00 x00 x02, x00 x00 x00 x14bXYZ x00 x00 x02@ x00 x00 x00 x14dmnd x00 x00 x02T x00 x00 x00pdmdd x00 x00 x02 xc4 x00 x00 x00 x88vued x00 x00 x03L x00 x00 x00 x86view x00 x00 x03 xd4 x00 x00 x00$lumi x00 x00 x03 xf8 x00 x00 x00 x14meas x00 x00 x04 x0c x00 x00 x00$tech x00 x00 x040 x00 x00 x00 x0crTRC x00 x00 x04&lt; x00 x00 x08 x0cgTRC x00 x00 x04&lt; x00 x00 x08 x0cbTRC x00 x00 x04&lt; x00 x00 x08 x0ctext x00 x00 x00 x00Copyright (c) 1998 Hewlett-Packard Company x00 x00desc x00 x00 x00 x00 x00 x00 x00 x12sRGB IEC61966-2.1 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x12sRGB IEC61966-2.1 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00XYZ x00 x00 x00 x00 x00 x00 xf3Q x00 x01 x00 x00 x00 x01 x16 xccXYZ x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00XYZ x00 x00 x00 x00 x00 x00o xa2 x00 x008 xf5 x00 x00 x03 x90XYZ x00 x00 x00 x00 x00 x00b x99 x00 x00 xb7 x85 x00 x00 x18 xdaXYZ x00 x00 x00 x00 x00 x00$ xa0 x00 x00 x0f x84 x00 x00 xb6 xcfdesc x00 x00 x00 x00 x00 x00 x00 x16IEC http://www.iec.ch x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x16IEC http://www.iec.ch x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00desc x00 x00 x00 x00 x00 x00 x00.IEC 61966-2.1 Default RGB colour space - sRGB x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00.IEC 61966-2.1 Default RGB colour space - sRGB x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00desc x00 x00 x00 x00 x00 x00 x00,Reference Viewing Condition in IEC61966-2.1 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00,Reference Viewing Condition in IEC61966-2.1 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00view x00 x00 x00 x00 x00 x13 xa4 xfe x00 x14_. x00 x10 xcf x14 x00 x03 xed xcc x00 x04 x13 x0b x00 x03 x9e x00 x00 x00 x01XYZ x00 x00 x00 x00 x00L tV x00P x00 x00 x00W x1f xe7meas x00 x00 x00 x00 x00 x00 x00 x01 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x00 x02 x8f x00 x00 x00 x02sig x00 x00 x00 x00CRT curv x00 x00 x00 x00 x00 x00 x04 x00 x00 x00 x00 x05 x00 n x00 x0f x00 x14 x00 x19 x00 x1e x00# x00( x00- x002 x007 x00; x00@ x00E x00J x00O x00T x00Y x00^ x00c x00h x00m x00r x00w x00| x00 x81 x00 x86 x00 x8b x00 x90 x00 x95 x00 x9a x00 x9f x00 xa4 x00 xa9 x00 xae x00 xb2 x00 xb7 x00 xbc x00 xc1 x00 xc6 x00 xcb x00 xd0 x00 xd5 x00 xdb x00 xe0 x00 xe5 x00 xeb x00 xf0 x00 xf6 x00 xfb x01 x01 x01 x07 x01 r x01 x13 x01 x19 x01 x1f x01% x01+ x012 x018 x01&gt; x01E x01L x01R x01Y x01` x01g x01n x01u x01| x01 x83 x01 x8b x01 x92 x01 x9a x01 xa1 x01 xa9 x01 xb1 x01 xb9 x01 xc1 x01 xc9 x01 xd1 x01 xd9 x01 xe1 x01 xe9 x01 xf2 x01 xfa x02 x03 x02 x0c x02 x14 x02 x1d x02&amp; x02/ x028 x02A x02K x02T x02] x02g x02q x02z x02 x84 x02 x8e x02 x98 x02 xa2 x02 xac x02 xb6 x02 xc1 x02 xcb x02 xd5 x02 xe0 x02 xeb x02 xf5 x03 x00 x03 x0b x03 x16 x03! x03- x038 x03C x03O x03Z x03f x03r x03~ x03 x8a x03 x96 x03 xa2 x03 xae x03 xba x03 xc7 x03 xd3 x03 xe0 x03 xec x03 xf9 x04 x06 x04 x13 x04 x04- x04; x04H x04U x04c x04q x04~ x04 x8c x04 x9a x04 xa8 x04 xb6 x04 xc4 x04 xd3 x04 xe1 x04 xf0 x04 xfe x05 r x05 x1c x05+ x05: x05I x05X x05g x05w x05 x86 x05 x96 x05 xa6 x05 xb5 x05 xc5 x05 xd5 x05 xe5 x05 xf6 x06 x06 x06 x16 x06 &#39; x067 x06H x06Y x06j x06{ x06 x8c x06 x9d x06 xaf x06 xc0 x06 xd1 x06 xe3 x06 xf5 x07 x07 x07 x19 x07+ x07= x07O x07a x07t x07 x86 x07 x99 x07 xac x07 xbf x07 xd2 x07 xe5 x07 xf8 x08 x0b x08 x1f x082 x08F x08Z x08n x08 x82 x08 x96 x08 xaa x08 xbe x08 xd2 x08 xe7 x08 xfb t x10 t% t: tO td ty t x8f t xa4 t xba t xcf t xe5 t xfb n x11 n &#39; n= nT nj n x81 n x98 n xae n xc5 n xdc n xf3 x0b x0b x0b&#34; x0b9 x0bQ x0bi x0b x80 x0b x98 x0b xb0 x0b xc8 x0b xe1 x0b xf9 x0c x12 x0c* x0cC x0c x0cu x0c x8e x0c xa7 x0c xc0 x0c xd9 x0c xf3 r r r&amp; r@ rZ rt r x8e r xa9 r xc3 r xde r xf8 x0e x13 x0e. x0eI x0ed x0e x7f x0e x9b x0e xb6 x0e xd2 x0e xee x0f t x0f% x0fA x0f^ x0fz x0f x96 x0f xb3 x0f xcf x0f xec x10 t x10&amp; x10C x10a x10~ x10 x9b x10 xb9 x10 xd7 x10 xf5 x11 x13 x111 x11O x11m x11 x8c x11 xaa x11 xc9 x11 xe8 x12 x07 x12&amp; x12E x12d x12 x84 x12 xa3 x12 xc3 x12 xe3 x13 x03 x13# x13C x13c x13 x83 x13 xa4 x13 xc5 x13 xe5 x14 x06 x14 &#39; x14I x14j x14 x8b x14 xad x14 xce x14 xf0 x15 x12 x154 x15V x15x x15 x9b x15 xbd x15 xe0 x16 x03 x16&amp; x16I x16l x16 x8f x16 xb2 x16 xd6 x16 xfa x17 x1d x17A x17e x17 x89 x17 xae x17 xd2 x17 xf7 x18 x1b x18@ x18e x18 x8a x18 xaf x18 xd5 x18 xfa x19 x19E x19k x19 x91 x19 xb7 x19 xdd x1a x04 x1a* x1aQ x1aw x1a x9e x1a xc5 x1a xec x1b x14 x1b; x1bc x1b x8a x1b xb2 x1b xda x1c x02 x1c* x1cR x1c{ x1c xa3 x1c xcc x1c xf5 x1d x1e x1dG x1dp x1d x99 x1d xc3 x1d xec x1e x16 x1e@ x1ej x1e x94 x1e xbe x1e xe9 x1f x13 x1f&gt; x1fi x1f x94 x1f xbf x1f xea x15 A l x98 xc4 xf0! x1c!H!u! xa1! xce! xfb&#34; &#39;&#34;U&#34; x82&#34; xaf&#34; xdd# n#8#f# x94# xc2# xf0$ x1f$M$|$ xab$ xda% t%8%h% x97% xc7% xf7&amp; &#39;&amp;W&amp; x87&amp; xb7&amp; xe8 &#39; x18 &#39;I &#39;z &#39; xab &#39; xdc( r(?(q( xa2( xd4) x06)8)k) x9d) xd0* x02*5*h* x9b* xcf+ x02+6+i+ x9d+ xd1, x05,9,n, xa2, xd7- x0c-A-v- xab- xe1. x16.L. x82. xb7. xee/$/Z/ x91/ xc7/ xfe050l0 xa40 xdb1 x121J1 x821 xba1 xf22*2c2 x9b2 xd43 r3F3 x7f3 xb83 xf14+4e4 x9e4 xd85 x135M5 x875 xc25 xfd676r6 xae6 xe97$7`7 x9c7 xd78 x148P8 x8c8 xc89 x059B9 x7f9 xbc9 xf9:6:t: xb2: xef;-;k; xaa; xe8&lt; &#39;&lt;e&lt; xa4&lt; xe3=&#34;=a= xa1= xe0&gt; &gt;`&gt; xa0&gt; xe0?!?a? xa2? xe2@#@d@ xa6@ xe7A)AjA xacA xeeB0BrB xb5B xf7C:C}C xc0D x03DGD x8aD xceE x12EUE x9aE xdeF&#34;FgF xabF xf0G5G{G xc0H x05HKH x91H xd7I x1dIcI xa9I xf0J7J}J xc4K x0cKSK x9aK xe2L*LrL xbaM x02MJM x93M xdcN%NnN xb7O x00OIO x93O xddP &#39;PqP xbbQ x06QPQ x9bQ xe6R1R|R xc7S x13S_S xaaS xf6TBT x8fT xdbU(UuU xc2V x0fV V xa9V xf7WDW x92W xe0X/X}X xcbY x1aYiY xb8Z x07ZVZ xa6Z xf5[E[ x95[ xe5 5 x86 xd6] &#39;]x] xc9^ x1a^l^ xbd_ x0f_a_ xb3` x05`W` xaa` xfcaOa xa2a xf5bIb x9cb xf0cCc x97c xebd@d x94d xe9e=e x92e xe7f=f x92f xe8g=g x93g xe9h?h x96h xeciCi x9ai xf1jHj x9fj xf7kOk xa7k xfflWl xafm x08m`m xb9n x12nkn xc4o x1eoxo xd1p+p x86p xe0q:q x95q xf0rKr xa6s x01s]s xb8t x14tpt xccu(u x85u xe1v&gt;v x9bv xf8wVw xb3x x11xnx xccy*y x89y xe7zFz xa5{ x04{c{ xc2|!| x81| xe1}A} xa1~ x01~b~ xc2 x7f# x7f x84 x7f xe5 x80G x80 xa8 x81 n x81k x81 xcd x820 x82 x92 x82 xf4 x83W x83 xba x84 x1d x84 x80 x84 xe3 x85G x85 xab x86 x0e x86r x86 xd7 x87; x87 x9f x88 x04 x88i x88 xce x893 x89 x99 x89 xfe x8ad x8a xca x8b0 x8b x96 x8b xfc x8cc x8c xca x8d1 x8d x98 x8d xff x8ef x8e xce x8f6 x8f x9e x90 x06 x90n x90 xd6 x91? x91 xa8 x92 x11 x92z x92 xe3 x93M x93 xb6 x94 x94 x8a x94 xf4 x95_ x95 xc9 x964 x96 x9f x97 n x97u x97 xe0 x98L x98 xb8 x99$ x99 x90 x99 xfc x9ah x9a xd5 x9bB x9b xaf x9c x1c x9c x89 x9c xf7 x9dd x9d xd2 x9e@ x9e xae x9f x1d x9f x8b x9f xfa xa0i xa0 xd8 xa1G xa1 xb6 xa2&amp; xa2 x96 xa3 x06 xa3v xa3 xe6 xa4V xa4 xc7 xa58 xa5 xa9 xa6 x1a xa6 x8b xa6 xfd xa7n xa7 xe0 xa8R xa8 xc4 xa97 xa9 xa9 xaa x1c xaa x8f xab x02 xabu xab xe9 xac xac xd0 xadD xad xb8 xae- xae xa1 xaf x16 xaf x8b xb0 x00 xb0u xb0 xea xb1` xb1 xd6 xb2K xb2 xc2 xb38 xb3 xae xb4% xb4 x9c xb5 x13 xb5 x8a xb6 x01 xb6y xb6 xf0 xb7h xb7 xe0 xb8Y xb8 xd1 xb9J xb9 xc2 xba; xba xb5 xbb. xbb xa7 xbc! xbc x9b xbd x15 xbd x8f xbe n xbe x84 xbe xff xbfz xbf xf5 xc0p xc0 xec xc1g xc1 xe3 xc2_ xc2 xdb xc3X xc3 xd4 xc4Q xc4 xce xc5K xc5 xc8 xc6F xc6 xc3 xc7A xc7 xbf xc8= xc8 xbc xc9: xc9 xb9 xca8 xca xb7 xcb6 xcb xb6 xcc5 xcc xb5 xcd5 xcd xb5 xce6 xce xb6 xcf7 xcf xb8 xd09 xd0 xba xd1&lt; xd1 xbe xd2? xd2 xc1 xd3D xd3 xc6 xd4I xd4 xcb xd5N xd5 xd1 xd6U xd6 xd8 xd7 xd7 xe0 xd8d xd8 xe8 xd9l xd9 xf1 xdav xda xfb xdb x80 xdc x05 xdc x8a xdd x10 xdd x96 xde x1c xde xa2 xdf) xdf xaf xe06 xe0 xbd xe1D xe1 xcc xe2S xe2 xdb xe3c xe3 xeb xe4s xe4 xfc xe5 x84 xe6 r xe6 x96 xe7 x1f xe7 xa9 xe82 xe8 xbc xe9F xe9 xd0 xea[ xea xe5 xebp xeb xfb xec x86 xed x11 xed x9c xee( xee xb4 xef@ xef xcc xf0X xf0 xe5 xf1r xf1 xff xf2 x8c xf3 x19 xf3 xa7 xf44 xf4 xc2 xf5P xf5 xde xf6m xf6 xfb xf7 x8a xf8 x19 xf8 xa8 xf98 xf9 xc7 xfaW xfa xe7 xfbw xfc x07 xfc x98 xfd) xfd xba xfeK xfe xdc xffm xff xff&#39;, &#39;Title&#39;: &#39;Eiffel Tower&#39;, &#39;Author&#39;: &#39;Shaurya GOel&#39;, &#39;Year&#39;: &#39;2023&#39;} . PARADIGM EXPLANATION: . In the aforementioned hacks, I made an effort to use redscale (a different byte manipulation technique), print metadata (vital information about the image), and alter the image&#39;s size using scaling techniques. I used a photograph I took, Baltimore.jpg, and used the red img program to convert it to a redscale image by setting the green and blue channels to 0 (in &quot;RGB&quot;). Three printed pieces of image metadata were also added by me. This information contained the image&#39;s title, the photographer&#39;s name, and the year it was shot. When a jpg/jpeg file is compressed, additional metadata with the notation &quot;jfif&quot; is present. . TWO IMAGES REPRESENTING LOSSY AND LOSSLESS COMPRESSION: . The lossy compression example will be the lassen-volcano.jpg image. This is owing to the fact that image file compression will undoubtedly result in a significantly smaller file size, but it will also result in a decrease in image quality because of the image&#39;s fine details and range of colors. It&#39;s probably going to get fuzzier.On the other hand, the green-square-16.png image can be used as an illustration of lossless compression. Because there is only one color in that image, the RGB pixel values will always be the same (without any intentional alteration of the image), the image quality will not change even though the image file size will reduce. .",
            "url": "https://stg-7.github.io/FastPages/2.2hacks.html",
            "relUrl": "/2.2hacks.html",
            "date": " • Mar 7, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "MCQ Test Blog 1/30/2023",
            "content": "MCQ Reflection and Blog . Overall score . . I got questions 17,29, and 50 wrong. | . ## Correcting Wrong Answers . ### Question 17 . . You had to choose one of two answers to this question. I chose the first option (A) correctly, but I chose Option B in error. When I reread the question, I saw that Option D contained a nested if-else statement that, unlike Option B, produced the same outcome as the question’s initial description (val1 &amp; not val2). . Question 29 . . The correct answer is A not B, because as it roates three times it will be facing downward. Furthermore, it will be in middle of the grid as the code goes to completion. . Question 50 . . The answer to the above-given question is option D. - Letter V . Further information and research on ASCII character encoding: . An international standard for encoding the Latin script was developed with the help of the American Standard Code for Information Interchange (ASCII). ASCII was first established in 1963 to enable the interpretation of data between computers. Symbols, numbers, lowercase and uppercase letters, and some instructions are represented. A radix-2 number system needs 7 bits since ASCII is encoded using 1s and 0s. Characters can be encoded using 2 to the power of 7 = 128 different number combinations with 7 bits. . Since ASCII only uses 7 bits to encode characters, using 8 bits in computing technology required the inclusion of additional bits. With this extra digit, extended ASCII up to 256 characters can be encoded. . As can be seen in the ASCII table above, ASCII encodes the specified 128 characters as 7-bit integers and was initially based on the (modern) English alphabet. One can print 95 of the encoded characters. These consist of the numbers 0 through 9, the letters lowercase a through z, uppercase a through z, and punctuation. The original ASCII specification also included 33 teletype machine-derived control codes that were not readable. Although the most of these are no longer in use, some are. Tab, line feed, and carriage return codes. . A lowercase letter, for instance, is represented by 1101001 in binary, 69 in hex (where I is the ninth character), and 105 in decimal in the ASCII encoding. American standard, however the cent () doesn’t have a code point in ASCII. Additionally, it does not accept proper nouns with diacritical marks, such as Beyoncé, or English phrases with diacritical marks, such resume or jalapeo. . Overall Reflection . Overall, I’m pleased with my grade because it demonstrates both my knowledge of the material covered in this class thus far and my ability to respond intelligently to multiple-choice questions of this type. The questions I misunderstood were also a teaching moment for me because they helped me understand how to approach issues of that nature going forward to avoid repeating my errors. Given that they nearly always result in real learning experiences for me, I enjoy having the chance to test myself on what we have studied so far and see how well I do. These kinds of exams let me know where my areas of strength and weakness lie and how I may strengthen those areas. I sincerely hope that when it comes time for this trimester’s multiple-choice final, I will demonstrate significant progress in how I respond to MCQ questions. .",
            "url": "https://stg-7.github.io/FastPages/markdown/2023/01/31/mcCB.html",
            "relUrl": "/markdown/2023/01/31/mcCB.html",
            "date": " • Jan 31, 2023"
        }
        
    
  
    
        ,"post4": {
            "title": "Physics Lab Report",
            "content": "Vardaan Sinha . Mr. Liao . AP Physics . 18 August 2022 . Lab #1: Measurements &amp; Graphical Analysis . Purpose Statement: The purpose of the lab is to find the relationship between the mass and radius of the disks and to calculate the uncertainty of the measured values through graphical analysis methods, such as linearization, where the graph shows a directly proportional relationship between the independent and dependent variables. . Derived Equation: . p = m/v → m = vp . | V = ah . | m = ( pi r^{2})hp . This equation shows that the mass of the disk is equal to the surface area of the disk (in other words, the area of a circle) multiplied by the height of the disk by the density of the disk. . | | Experiment Outline: . Measure the mass of the disk using the balance. Before using the balance, it must be zeroed, and then the disk can be placed on the balance. . | Measure the radius of the disk using the meter stick. The radius is measured by placing one end of the meter stick in the center of the circle, and the other end on the outside edge of the circle. . | Repeat Steps 1 &amp; 2 for each disk. . | Measure the thickness of one of the disks by folding the disk and measuring the height. Since the disk is folded to make it easier to find the height, divide the measurement appropriately to find the height if the disk was not folded. . | Graph the proportional mass vs radius2 relationship and determine the equation for the best-fit line. . | Precision: . The meter stick’s precision our group used was 1 millimeter (0.1 centimeters), since the meter stick used to measure the radius of the disks had markings down to the tenth of a centimeter. The measured radii of the disks in the data table is noted to the nearest hundredth of a centimeter, because of the 2/5/8 rounding rule, which added an extra digit of precision to the measurement. . | Data Table: . Radius2 (cm2) vs. Mass (grams) &amp; Radius (cm) . Radius (centimeters) Mass (grams)   Radius2 (centimeters2) . 5.32 cm | 0.37 g |   | 28.3024 cm2 | . 6.92 cm | 0.71 g |   | 47.8864 cm2 | . 4.12 cm | 0.25 g |   | 16.9744 cm2 | . 3.08 cm | 0.13 g |   | 9.4864 cm2 | . 2.28 cm | 0.08 g |   | 5.1984 cm2 | . . This is a nonlinear regression. . Equation: y = 0.0199151x2 - 0.0499334x + 0.095349 . . This is a linear regression. . Line of Best Fit Equation: y = 0.0146174x - 0.00729059 . Analysis Questions: . From the line of best fit, the independent variable represents the radius2 (centimeters2). This is because there is a linear relationship between the measured mass of each disk in grams and the corresponding squared radius values (in centimeters2) for each disk. . | The slope of the line of best fit has units of grams/cm2. In the original equation given in the lab, mass is equal to density multiplied by the disk’s surface area multiplied by the height of the disk. The surface area of the disk can be calculated through the formula for the area of the circle, or pi multiplied by the radius squared. Since the density, pi, and height values are all constant, they make up the coefficient for the slope of the line of best fit equation. Density is measured in grams/centimeter3, pi is a numerical constant, and height is measured in centimeters, and multiplying these values together gives units of grams/centimeter2. To verify that these are the correct units, the remaining part of the original equation is radius2, which has units of centimeters2, and multiplying the slope units (grams/centimeter2) by centimeters2 would produce a value in grams, which is the unit of measurement for m (mass). Since the y-value of the line of best fit equation has units of grams, the y-intercept (-0.00729059) must also be with units of grams, in order to produce the same unit on both sides of the equation: . In this case, g represents grams, and cm represents centimeters. . y = mx + b . | g = (g/cm2 * cm2) + g . | g = g + g . | g = g (verification) . | | | Yes, in ideal conditions, the “b” in the y = mx + b formula should be zero, as a disk with a radius of 0 centimeters would have a mass of 0 grams. . | Best-Fit Line Slope: 0.0146174 . The “thickness” of the cylinders is approximately 0.00125 centimeters. . | Density * ( pi) * Height = Slope . | Density * ( pi) * 0.00125 cm = 0.0146174 . | Density * 0.003925 cm = 0.0146174 . | Density = 3.72417834 ( approx) 3.72 g/cm3 . | The actual density of aluminum is 2.7 g/cm3. . | Percent Error: (3.72 g/cm3 - 2.7 g/cm3)/(2.7 g/cm3) x 100% = 37.778% ( approx) 38% . | | Our calculated density for the aluminum foil disk (3.72 g/cm3) was slightly larger than the actual calculated density of aluminum (2.7 g/cm3), as shown by our positive 38% error. Our number was likely larger than the actual value because of the inability to accurately measure the height of each disk, leading to an under measurement, creating a higher density value than expected. Furthermore, the measurement of the disk area may have been slightly inaccurate due to each disk not being perfectly circular. . | Synthesis Questions: . Disks with a greater thickness would correspond to an increase in the slope of the best-fit line. A greater thickness with an unchanged radius would result in a greater volume of the disk, leading to an increase in the slope of the line. The experimental density would stay the same, since the increased thickness would be reflected in the increased slope of the line. The slope is equal to the density multiplied by pi (constant) by the thickness of the disk, and with the increased thickness already accounted for in the increased slope, the density would remain the same as before. Disks with a greater thickness would have allowed for a more accurate measurement of the height, creating a more accurate density measurement. . | In this modified graph, the purple line (also circled) represents a scenario in which steel disks with the same thickness were used in the experiment. The slope for the line representing the steel disks is steeper than the line for the aluminum foil disks, since the density of steel (7.85 g/cm3) is much greater than that of aluminum (2.7 g/cm3), and the slope is directly proportional to the density. The purple line in the above graph has a slope of approximately 2.9 times greater than the best fit line showing the relationship between mass and radius2, as the density of steel is 2.9 times greater than that of aluminum. . | Work: . Slope: 122 kg/m2 . | Thickness: 0.5 cm . | Density * ( pi) * Thickness = Slope . | Density * ( pi) * 0.5 cm = 122 kg/m2 . | Density * 1.57 cm = 122 kg/m2 . | 122 kg/m2 x (1.0 m2/10,000 cm2) x (1000 g/1.0 kg) = 12.2 g/cm2 . | Density * 1.57 cm = 12.2 g/cm2 . | Density = 7.77 g/cm3 . | This is closest to the density of iron, which is 7.8 g/cm3. Therefore, the unknown material that these disks are made of is iron. . | | Multiple Choice Questions: . Mass = Density * Volume . Density * Area * Height = Mass . | Density * ( pi)r2 * Height = Mass . | C = 2( pi)r . | C2 = 2( pi)r2 . | C2/(2( pi)) = r2 . | Density * ( pi)(C2/(2( pi))2) * Height = Mass . | Mass = (Density * Height * 1/4( pi)) * C2 . | Slope = (Density * Height)/(4( pi)) . | Therefore, E is the correct option. . | | SKIP . | Volume of a Sphere = 4/3( pi)r3 (r = radius) . Mass = Density * Volume . | Mass = Density * 4/3( pi)r3 . | Mass = (Density * 4/3( pi)) x r3 . | Since density and 4/3( pi) are constants, the mass would need to be plotted versus the radius3, so the correct option is C. . | |",
            "url": "https://stg-7.github.io/FastPages/2022/08/28/Physics-Lab-Report.html",
            "relUrl": "/2022/08/28/Physics-Lab-Report.html",
            "date": " • Aug 28, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://stg-7.github.io/FastPages/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  
      ,"page0": {
          "title": "About Me",
          "content": "Hello, I’m Shaurya Goel. .",
          "url": "https://stg-7.github.io/FastPages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://stg-7.github.io/FastPages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}